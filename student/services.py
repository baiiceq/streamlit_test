import logging

from database import Database
import uuid
from datetime import datetime,timedelta
from tool import chat_agent, analysis
import streamlit as st
import json
from core.models import Class
import pandas as pd
from tool.study_report import LearningReportGenerator
import os
from fpdf import FPDF

class ConversationService:
    def __init__(self, student_id):
        self.student_id = student_id
        self.conversation_manager = chat_agent.ConversationManager()

        self.title_prompt_template  = """
            è¯·é˜…è¯»ä»¥ä¸‹å¯¹è¯ï¼Œæå–ä¸€ä¸ªä¸è¶…è¿‡6ä¸ªå­—çš„é¢˜ç›®
            
            [å¯¹è¯è®°å½•]
            {coversation_history}
        """

        self.knowledge_points_template = """
            è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯è®°å½•ï¼Œä»çŸ¥è¯†ç‚¹åˆ—è¡¨ä¸­æå–å‡ºç›¸ç¬¦åˆçš„çŸ¥è¯†ç‚¹ï¼ˆ4ä¸ªä»¥å†…ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONè¿”å›(ä»…åŒ…å«pointså­—æ®µ)
            æå–çŸ¥è¯†ç‚¹å¿…é¡»æ˜¯çŸ¥è¯†ç‚¹åˆ—è¡¨ä¸­çš„ï¼Œä¸å…è®¸äº§ç”Ÿæ–°çŸ¥è¯†ç‚¹
            [çŸ¥è¯†ç‚¹åˆ—è¡¨]
            {points}
            
            [å¯¹è¯è®°å½•]
            {history}
            
            è¿”å›JSONå½¢å¼(ä»…åŒ…å«pointså­—æ®µ,åªéœ€è¦è¾“å‡ºçŸ¥è¯†ç‚¹çš„åå­—)
        """

    def load_history(self):
        """åŠ è½½å†å²å¯¹è¯"""
        return Database.load_conversation_history(self.student_id)

    def create_conversation(self):
        """åˆ›å»ºæ–°å¯¹è¯"""
        new_conversation = {
            "conversation_id": str(uuid.uuid4()),
            "student_id": self.student_id,
            "title": "æ–°å¯¹è¯",
            "messages": [],
            "knowledges": [],
            "created_at": datetime.now(),
            "updated_at": datetime.now(),
        }
        return new_conversation

    def save_conversation(self, conversation):
        """ä¿å­˜å¯¹è¯"""
        Database.save_conversation(conversation)

    def get_ai_response(self, prompt):
        use_web_search = st.session_state["web_search"]
        if(st.session_state["deep_thought"]):
            ai_response, web_summary, thought_text = self.conversation_manager.process_conversation(
                prompt, use_web_search=use_web_search
            )
        else:
            ai_response, web_summary, reasoning_text = self.conversation_manager.thinking_input(
                prompt, use_web_search=use_web_search
            )
        return ai_response

    def update_coversation_title(self):
        length = len(st.session_state.current_conversation["messages"])
        if  length <= 2 or  length % 6 ==0:
            last_msgs = " ".join(
                [m["content"] for m in st.session_state.current_conversation["messages"][-6:]])
            title_prompt = self.title_prompt_template.format(coversation_history=last_msgs)
            new_title = self.conversation_manager.get_direct_response(title_prompt)
            st.session_state.current_conversation["title"] = new_title

    def get_knowledge_points(self, question, answer):
        message = f"student : {question}\nassitent : {answer}"

        with open("c_language.json", 'r', encoding='utf-8') as f:
            points = json.dumps(json.load(f), ensure_ascii=False, indent=4)
        points_prompt = self.knowledge_points_template.format(history=message,points=points)
        response = self.conversation_manager.get_direct_response(points_prompt)

        try:
            cleaned = response.replace("```json", "").replace("json", "").replace("```", "").strip()
            data = json.loads(cleaned)
            self.save_knowlegdes_points(data["points"])
        except:
            logging.error("çŸ¥è¯†ç‚¹æå–å¤±è´¥")

    def save_knowlegdes_points(self, points):
        st.session_state.current_conversation["knowledges"].append((datetime.now(),points))


    def clear_memory(self):
        self.conversation_manager.clear_memory()



class ReportService:
    def __init__(self, student_id):
        self.student_id = student_id
        self.knowledge_points = None
        self.report_service_init(datetime.now()-timedelta(weeks=1),datetime.now())
        self.learning_report_generator = LearningReportGenerator(os.environ.get("DASHSCOPE_API_KEY"))

    def report_service_init(self, start_date, end_date):
        self.knowledge_points = Database.load_knowledge_points_by_date(self.student_id, start_date, end_date)
        self.messages = Database.load_coversations_by_date(self.student_id, start_date, end_date)
        self.df = pd.DataFrame(self.knowledge_points, columns=['timestamp', 'knowledge_points'])
        self.data = []
        for points in self.knowledge_points:
            self.data.append(points[1])
        self.freq = analysis.knowledge_frequency_analysis(self.data)

    def messages_to_json(self):
        message_list = []
        for i in range(0, len(self.messages), 2):
            message_list.append({
                "timestamp": self.messages[i]["timestamp"].strftime("%Y-%m-%d %H:%M:%S"),
                "question": self.messages[i]["content"],
                "answer": ""
            })

        # è½¬æ¢ä¸º JSON æ ¼å¼å­—ç¬¦ä¸²
        conversation_json = json.dumps(message_list, indent=4, ensure_ascii=False)

        return conversation_json

    def generator_report(self):
        json_text = self.messages_to_json()

        with st.spinner("æ­£åœ¨ç”Ÿæˆå­¦ä¹ æŠ¥å‘Šï¼Œè¯·ç¨å€™..."):
            final_report, keywords = self.learning_report_generator.generate_report(json_text)
            if final_report:
                st.markdown("### ç”Ÿæˆçš„å­¦ä¹ æŠ¥å‘Š")
                print(final_report)
                st.markdown(final_report)
                st.success("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
                st.session_state["keywords"] = keywords
                # è®°å½•æŠ¥å‘Šï¼šç¼–å·ä¸º æ—¥æœŸ+åºå·
                today_str = datetime.now().strftime("%Y%m%d")
                count_today = sum(
                    1 for report in st.session_state.report_history if report["id"].startswith(today_str)) + 1
                report_id = f"{today_str}-{count_today:03d}"
                timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                st.session_state.report_history.append({
                    "id": report_id,
                    "timestamp": timestamp_str,
                    "report": final_report
                })
                # PDF å¯¼å‡ºï¼Œä½¿ç”¨æ”¯æŒ Unicode çš„å­—ä½“
                pdf = FPDF()
                pdf.add_page()
                font_path = os.path.join(os.path.dirname(__file__), "chinese.simhei.ttf")
                pdf.add_font("DejaVu", "", font_path, uni=True)
                pdf.set_font("DejaVu", size=12)
                for line in final_report.split('\n'):
                    pdf.multi_cell(0, 10, line)
                pdf_output = pdf.output(dest="S").encode("latin1", errors="replace")
                st.download_button("ä¸‹è½½ PDF", data=pdf_output, file_name=f"{report_id}.pdf",
                                   mime="application/pdf")

    def knowledge_points_freqency_report(self):
        st.header("2ï¸âƒ£ çŸ¥è¯†ç‚¹é¢‘ç‡åˆ†æ")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“‹ çŸ¥è¯†ç‚¹é¢‘ç‡è¡¨æ ¼")
            analysis.plot_frequency_table(self.freq)
        with col2:
            st.subheader("â˜ï¸ çŸ¥è¯†ç‚¹è¯äº‘å›¾")
            analysis.plot_frequency_wordcloud_streamlit(self.freq)
        st.markdown("---")
        st.subheader("3ï¸âƒ£ çŸ¥è¯†ç‚¹é¢‘ç‡TOP10å›¾è¡¨")
        chart_type = st.radio("é€‰æ‹©å›¾è¡¨ç±»å‹ï¼š", options=["æŸ±çŠ¶å›¾", "é¥¼å›¾"], horizontal=True, key="chart_type")
        if chart_type == "æŸ±çŠ¶å›¾":
            analysis.plot_top_frequency_bar(self.freq, top_n=10)
        else:
            analysis.plot_top_frequency_pie(self.freq, top_n=10)

    def collinear_report(self):
        st.header("ğŸ”„ çŸ¥è¯†ç‚¹å…±ç°åˆ†æ")
        with st.expander("ğŸ“– åˆ†æè¯´æ˜ä¸è§£è¯»æŒ‡å—", expanded=True):
            st.markdown("""
                               ### å¦‚ä½•è§£è¯»å…±ç°åˆ†æï¼Ÿ
                               1. **å…±ç°é¢‘ç‡è¡¨**ï¼šæ˜¾ç¤ºçŸ¥è¯†ç‚¹ä¸¤ä¸¤ç»„åˆçš„å‡ºç°æ¬¡æ•°ï¼Œé«˜é¢‘ç»„åˆæç¤ºæ•™å­¦ä¸­çš„å¸¸è§çŸ¥è¯†å…³è”
                               2. **çƒ­åŠ›å›¾**ï¼šé¢œè‰²è¶Šæ·±è¡¨ç¤ºå…±ç°é¢‘ç‡è¶Šé«˜ï¼Œå¯¹è§’çº¿æ˜¾ç¤ºå•ä¸ªçŸ¥è¯†ç‚¹å‡ºç°é¢‘æ¬¡
                               3. **ç½‘ç»œå›¾**ï¼š
                                  - èŠ‚ç‚¹å¤§å°åæ˜ çŸ¥è¯†ç‚¹å‡ºç°é¢‘ç‡
                                  - è¿çº¿ç²—ç»†è¡¨ç¤ºå…±ç°å¼ºåº¦
                                  - ç´«è‰²èŠ‚ç‚¹è¡¨ç¤ºæ ¸å¿ƒæ¢çº½çŸ¥è¯†ç‚¹
                                  - ç´§å¯†è¿æ¥çš„ç¾¤è½æç¤ºçŸ¥è¯†æ¨¡å—

                               ### æ•™å­¦åº”ç”¨ä»·å€¼ï¼š
                               âœ… å‘ç°é«˜é¢‘ç»„åˆ â†’ ä¼˜åŒ–è¯¾ç¨‹è®¾è®¡ä¸­çš„çŸ¥è¯†ç‚¹æ­é…  
                               âœ… è¯†åˆ«æ ¸å¿ƒèŠ‚ç‚¹ â†’ åŠ å¼ºé‡ç‚¹çŸ¥è¯†ç‚¹çš„æ•™å­¦  
                               âœ… å‘ç°çŸ¥è¯†ç¾¤è½ â†’ å»ºç«‹æ¨¡å—åŒ–æ•™å­¦ä½“ç³»  
                               âœ… å®šä½è–„å¼±ç¯èŠ‚ â†’ å‘ç°åº”åŠ å¼ºå…³è”çš„æ•™å­¦ç‚¹

                               *ç¤ºä¾‹ï¼šè‹¥"ä¸‰è§’å‡½æ•°"ä¸"å‘é‡"é«˜é¢‘å…±ç°ï¼Œå»ºè®®åœ¨æ•™å­¦ä¸­å¼ºåŒ–äºŒè€…çš„ç»¼åˆåº”ç”¨è®­ç»ƒ*
                               """)
        analysis.analyze_knowledge_cooccurrence(self.data)
        st.markdown("---")
        st.subheader("ğŸ•¸ï¸ çŸ¥è¯†ç‚¹å…±ç°ç½‘ç»œ")
        col1, col2 = st.columns(2)
        with col1:
            min_cooccurrence = st.slider("æœ€å°å…±ç°é˜ˆå€¼", min_value=1, max_value=10, value=1,
                                         help="åªæ˜¾ç¤ºå…±ç°æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„çŸ¥è¯†ç‚¹å¯¹")
        with col2:
            max_nodes = st.slider("æœ€å¤§èŠ‚ç‚¹æ•°é‡", min_value=5, max_value=30, value=15,
                                  help="é™åˆ¶ç½‘ç»œå›¾ä¸­æ˜¾ç¤ºçš„çŸ¥è¯†ç‚¹æ•°é‡")
        analysis.build_knowledge_network(self.data, self.freq, min_cooccurrence, max_nodes)

    def timing_report(self):
        if 'timestamp' in self.df.columns:
            st.header("â° æ—¶åºè¶‹åŠ¿åˆ†æ")
            with st.expander("ğŸ” åˆ†æç»´åº¦è¯´æ˜", expanded=True):
                st.markdown("""
                ### æœ¬æ¨¡å—åˆ†æç»´åº¦åŒ…æ‹¬ï¼š
                1. **å­¦ä¹ ä¼šè¯åˆ†æ**ï¼šè¯†åˆ«è¿ç»­å­¦ä¹ æ—¶æ®µå’Œé—´éš”
                2. **å­¦ä¹ å¼ºåº¦åˆ†æ**ï¼šåˆ†ææ¯æ—¥/æ¯å‘¨å­¦ä¹ è§„å¾‹
                3. **çŸ¥è¯†ç„¦ç‚¹è¿ç§»**ï¼šè·Ÿè¸ªçŸ¥è¯†ç‚¹å…³æ³¨åº¦å˜åŒ–
                """)

            df_enriched, session_stats = analysis.analyze_learning_sessions(self.df)
            analysis.analyze_knowledge_learning_curve(df_enriched)
            analysis.analyze_learning_intensity(df_enriched)

        else:
            st.error("æ— æ³•è¿›è¡Œæ—¶åºåˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

    def time_preference_report(self):
        if 'timestamp' in self.df.columns:
            st.header("â±ï¸ å­¦ç”Ÿæé—®æ—¶é—´åå¥½åˆ†æ")
            analysis.analyze_time_preference(self.df)
        else:
            st.error("æ— æ³•è¿›è¡Œæ—¶é—´åˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

    def daily_points_report(self):
        if 'timestamp' in self.df.columns:
            st.header("ğŸ“ˆ æ¯æ—¥çŸ¥è¯†ç‚¹ç»„æˆåˆ†æ")
            analysis.analyze_daily_knowledge_composition(self.df)
        else:
            st.error("æ— æ³•è¿›è¡Œæ¯æ—¥åˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

    def causal_knowledge_report(self):
        if 'timestamp' in self.df.columns:
            analysis.analyze_causal_relationships(self.data)
        else:
            st.error("éœ€è¦æ—¶é—´æˆ³æ•°æ®è¿›è¡Œå› æœæ—¶åºåˆ†æ")

    def personalize_report(self):
        st.header("âœ¨ ä¸ªæ€§åŒ–åé¦ˆ")
        analysis.create_learning_profile(self.df, self.data)

    def depth_timing_report(self):
        analysis.advanced_time_series_analysis(self.df)

    def memory_report(self):
        if 'timestamp' in self.df.columns:
            analysis.analyze_memory_persistence(self.df.copy())
        else:
            st.error("éœ€è¦æ—¶é—´æˆ³æ•°æ®è¿›è¡Œè®°å¿†æŒä¹…æ€§åˆ†æ")

class ClassService:
    def __init__(self, student_id):
        self.student_id = student_id
        self.student_info = None

    def get_joined_classes(self):
        """è·å–å­¦ç”ŸåŠ å…¥çš„ç­çº§"""
        if self.student_info == None:
            self.student_info = Database.find_user({"student_id": self.student_id})

        return self.student_info.get("classes", [])

    def join_class(self, invite_code):
        """åŠ å…¥ç­çº§"""
        target_class = Class.find_by_invite_code(invite_code.upper())
        if target_class:
            if target_class.class_id in self.get_joined_classes():
                st.error("æ‚¨å·²åŠ å…¥è¯¥ç­çº§")
            else:
                try:
                    # æ·»åŠ å­¦ç”Ÿåˆ°ç­çº§
                    target_class.add_student(st.session_state.user["student_id"])
                    # æ›´æ–°å­¦ç”Ÿä¿¡æ¯
                    Database.add_user_class(st.session_state.user["user_id"], target_class.class_id)
                    st.success(f"æˆåŠŸåŠ å…¥ç­çº§ï¼š{target_class.class_name}")
                    st.rerun()
                except Exception as e:
                    st.error("åŠ å…¥ç­çº§å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
        else:
            st.error("æ— æ•ˆçš„é‚€è¯·ç ")

class ExerciseService:
    def __init__(self, student_id):
        self.student_id = student_id
