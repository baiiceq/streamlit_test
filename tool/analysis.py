import streamlit as st
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import altair as alt
from itertools import combinations
import networkx as nx
from wordcloud import WordCloud
import io
from pgmpy.estimators import PC
import networkx as nx
import os
import streamlit as st
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
from collections import Counter
import altair as alt
import plotly.express as px
import plotly.graph_objects as go
os.environ["DISABLE_STREAMLIT_WATCHER"] = "1"
# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']  # é€‚ç”¨äº Windows
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³åæ ‡è½´è´Ÿå·æ˜¾ç¤ºé—®é¢˜
from typing import Dict, Any
"""
å­¦ç”ŸçŸ¥è¯†ç‚¹åˆ†æå·¥å…·ä»£ç æ³¨é‡Šè¯´æ˜

æœ¬å·¥å…·æä¾›å¤šç»´åº¦çš„å­¦ç”ŸçŸ¥è¯†ç‚¹åˆ†æåŠŸèƒ½ï¼ŒåŒ…å«é¢‘ç‡åˆ†æã€å…±ç°åˆ†æã€æ—¶åºåˆ†æã€å› æœæ¨æ–­ç­‰æ¨¡å—ã€‚
å„æ¨¡å—å‡½æ•°å‡å¯ç‹¬ç«‹è°ƒç”¨ï¼Œè¾“å…¥æ•°æ®æ ¼å¼ç»Ÿä¸€ï¼Œè¾“å‡ºåŒ…å«å¯è§†åŒ–å›¾è¡¨å’Œç»“æ„åŒ–æ•°æ®ã€‚

ä¸»è¦æ•°æ®ç»“æ„è¯´æ˜ï¼š
- åŸå§‹æ•°æ®åº”ä¸ºåŒ…å«"knowledge_points"å’Œ"timestamp"åˆ—çš„DataFrame
- knowledge_pointsåˆ—æ ¼å¼ï¼šåˆ—è¡¨æˆ–é€—å·åˆ†éš”å­—ç¬¦ä¸²ï¼Œå¦‚["çŸ¥è¯†ç‚¹A", "çŸ¥è¯†ç‚¹B"] æˆ– "çŸ¥è¯†ç‚¹A, çŸ¥è¯†ç‚¹B"
- timestampåˆ—æ ¼å¼ï¼šå¯è½¬æ¢ä¸ºdatetimeçš„æ—¶é—´æˆ³å­—ç¬¦ä¸²

ä»£ç ç»“æ„ï¼š
1. åŸºç¡€ç»Ÿè®¡åˆ†ææ¨¡å—
2. æ—¶é—´åˆ†ææ¨¡å—
3. çŸ¥è¯†ç‚¹å…±ç°åˆ†ææ¨¡å—
4. å› æœæ¨æ–­æ¨¡å—
5. å­¦ä¹ è¡Œä¸ºåˆ†ææ¨¡å—
6. ä¸ªæ€§åŒ–åé¦ˆæ¨¡å—
7. è®°å¿†æŒä¹…æ€§åˆ†ææ¨¡å—
"""
# ---------------------------
# 1. åŸºç¡€ç»Ÿè®¡åˆ†ææ¨¡å—
# ---------------------------
def knowledge_frequency_analysis(data):
    """
    çŸ¥è¯†ç‚¹é¢‘ç‡ç»Ÿè®¡åˆ†æ

    å‚æ•°:
        data: list[list] - äºŒç»´åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨è¡¨ç¤ºå•æ¬¡æé—®æ¶‰åŠçš„çŸ¥è¯†ç‚¹
        ç¤ºä¾‹: [["ä¸‰è§’å½¢", "å‘é‡"], ["å‘é‡", "å¯¼æ•°"]]

    è¿”å›:
        Counterå¯¹è±¡ - å„çŸ¥è¯†ç‚¹å‡ºç°é¢‘æ¬¡
        ç¤ºä¾‹: Counter({"å‘é‡":2, "ä¸‰è§’å½¢":1, "å¯¼æ•°":1})

    åŠŸèƒ½:
        1. æ‰å¹³åŒ–å¤„ç†äºŒç»´çŸ¥è¯†ç‚¹åˆ—è¡¨
        2. ä½¿ç”¨Counterç»Ÿè®¡çŸ¥è¯†ç‚¹å‡ºç°æ¬¡æ•°
    """
    flat_list = [kp for record in data for kp in record]
    freq = Counter(flat_list)
    return freq

def plot_frequency_table(freq):
    """
    å¯è§†åŒ–çŸ¥è¯†ç‚¹é¢‘ç‡è¡¨æ ¼

    å‚æ•°:
        freq: Counterå¯¹è±¡ - çŸ¥è¯†ç‚¹é¢‘ç‡æ•°æ®

    åŠŸèƒ½:
        1. å°†Counterè½¬æ¢ä¸ºDataFrame
        2. ä½¿ç”¨Streamlitå±•ç¤ºæ’åºåçš„é¢‘ç‡è¡¨æ ¼
        3. æ”¯æŒåŠ¨æ€äº¤äº’æ’åº
    """
    df_freq = pd.DataFrame({
        'çŸ¥è¯†ç‚¹': list(freq.keys()),
        'å‡ºç°é¢‘æ¬¡': list(freq.values())
    }).sort_values(by='å‡ºç°é¢‘æ¬¡', ascending=False).reset_index(drop=True)

    st.dataframe(
        df_freq,
        column_config={
            "çŸ¥è¯†ç‚¹": st.column_config.TextColumn("çŸ¥è¯†ç‚¹"),
            "å‡ºç°é¢‘æ¬¡": st.column_config.NumberColumn("å‡ºç°é¢‘æ¬¡", format="%d")
        },
        use_container_width=True,
        hide_index=True
    )

def plot_top_frequency_bar(freq, top_n=10):
    """
    ç»˜åˆ¶çŸ¥è¯†ç‚¹é¢‘ç‡TOP10æŸ±çŠ¶å›¾

    å‚æ•°:
        freq: Counterå¯¹è±¡ - çŸ¥è¯†ç‚¹é¢‘ç‡æ•°æ®
        top_n: æ˜¾ç¤ºå‰Nä¸ªçŸ¥è¯†ç‚¹

    åŠŸèƒ½:
        1. æå–TOP NçŸ¥è¯†ç‚¹
        2. ä½¿ç”¨Altairç»˜åˆ¶äº¤äº’å¼æŸ±çŠ¶å›¾
        3. æ”¯æŒé¼ æ ‡æ‚¬åœæŸ¥çœ‹æ•°å€¼
    """
    top_items = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:top_n])
    df_chart = pd.DataFrame({
        'çŸ¥è¯†ç‚¹': list(top_items.keys()),
        'é¢‘æ¬¡': list(top_items.values())
    })

    chart = alt.Chart(df_chart).mark_bar().encode(
        x=alt.X('çŸ¥è¯†ç‚¹:N', sort='-y'),
        y=alt.Y('é¢‘æ¬¡:Q'),
        color=alt.Color('çŸ¥è¯†ç‚¹:N', legend=None),
        tooltip=['çŸ¥è¯†ç‚¹', 'é¢‘æ¬¡']
    ).properties(
        title='çŸ¥è¯†ç‚¹é¢‘ç‡TOP10',
        width=600,
        height=400
    ).interactive()

    st.altair_chart(chart, use_container_width=True)

def plot_top_frequency_pie(freq, top_n=10):
    top_items = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:top_n])
    fig = px.pie(
        names=list(top_items.keys()),
        values=list(top_items.values()),
        title='çŸ¥è¯†ç‚¹é¢‘ç‡TOP10å æ¯”',
        hole=0.3,
        color_discrete_sequence=px.colors.qualitative.Pastel
    )
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(uniformtext_minsize=12, uniformtext_mode='hide')
    st.plotly_chart(fig, use_container_width=True)

def plot_frequency_wordcloud_streamlit(freq):
    try:
        wc = WordCloud(
            font_path="msyh.ttc",  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            width=800,
            height=400,
            background_color='white',
            max_words=100,
            contour_width=1,
            contour_color='steelblue'
        )
        wc.generate_from_frequencies(freq)
        img = wc.to_image()
        buf = io.BytesIO()
        img.save(buf, format='PNG')
        st.image(buf, caption='çŸ¥è¯†ç‚¹è¯äº‘å›¾', use_column_width=True)
    except Exception as e:
        st.error(f"ç”Ÿæˆè¯äº‘å›¾æ—¶å‡ºé”™: {e}")
        st.info("æ£€æŸ¥å­—ä½“è·¯å¾„æ˜¯å¦æ­£ç¡®æˆ–ç³»ç»Ÿä¸­æ˜¯å¦å®‰è£…äº†'msyh.ttc'å­—ä½“ã€‚")

# ---------------------------
# 2. æ—¶é—´åˆ†ææ¨¡å—
# ---------------------------

def analyze_daily_knowledge_composition(df):
    """
    æ¯æ—¥çŸ¥è¯†ç‚¹ç»„æˆåˆ†æ

    å‚æ•°:
        df: åŒ…å«timestampå’Œknowledge_pointsåˆ—çš„DataFrame

    åŠŸèƒ½:
        1. æŒ‰å¤©èšåˆçŸ¥è¯†ç‚¹æ•°æ®
        2. ç»Ÿè®¡æ¯æ—¥çŸ¥è¯†ç‚¹åˆ†å¸ƒ
        3. ç»˜åˆ¶å †å æŸ±çŠ¶å›¾å±•ç¤ºæ¯æ—¥çŸ¥è¯†ç‚¹ç»„æˆå˜åŒ–
    """
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date

    daily_data = []
    for date, group in df.groupby('date'):
        day_kps = []
        for kps in group['knowledge_points']:
            # å¦‚æœçŸ¥è¯†ç‚¹å·²æ˜¯åˆ—è¡¨åˆ™ç›´æ¥æ·»åŠ ï¼Œå¦åˆ™æŒ‰é€—å·åˆ†å‰²
            if isinstance(kps, list):
                day_kps.extend(kps)
            else:
                day_kps.extend([kp.strip() for kp in kps.split(',')])
        kp_count = Counter(day_kps)
        top_kps = dict(kp_count.most_common(5))
        for kp, count in top_kps.items():
            daily_data.append({'æ—¥æœŸ': date, 'çŸ¥è¯†ç‚¹': kp, 'é¢‘æ¬¡': count})
    if not daily_data:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ—¥æœŸæ•°æ®è¿›è¡Œåˆ†æ")
        return

    df_daily = pd.DataFrame(daily_data)
    fig = px.bar(
        df_daily,
        x='æ—¥æœŸ',
        y='é¢‘æ¬¡',
        color='çŸ¥è¯†ç‚¹',
        title='æ¯æ—¥æé—®çŸ¥è¯†ç‚¹ç»„æˆ',
        barmode='stack'
    )
    fig.update_layout(
        xaxis_title='æ—¥æœŸ',
        yaxis_title='æé—®é¢‘æ¬¡',
        legend_title='çŸ¥è¯†ç‚¹',
        hovermode='x'
    )
    st.plotly_chart(fig, use_container_width=True,key="unique_chart_daily")

def analyze_time_preference(df):
    """
    å­¦ä¹ æ—¶é—´åå¥½åˆ†æ

    å‚æ•°:
        df: åŒ…å«timestampåˆ—çš„DataFrame

    åŠŸèƒ½:
        1. å°†æ—¶é—´åˆ’åˆ†ä¸ºä¸Šåˆ/ä¸‹åˆ/æ™šä¸Šä¸‰ä¸ªæ—¶æ®µ
        2. ç»Ÿè®¡å„æ—¶æ®µæé—®æ•°é‡åˆ†å¸ƒ
        3. è¯†åˆ«å­¦ä¹ é«˜å³°æ—¶æ®µ
        4. ç”Ÿæˆæ—¶é—´ç®¡ç†å»ºè®®
    """
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour'] = df['timestamp'].dt.hour
    def get_time_period(hour):
        if 5 <= hour < 12:
            return 'ä¸Šåˆ (5:00-11:59)'
        elif 12 <= hour < 18:
            return 'ä¸‹åˆ (12:00-17:59)'
        else:
            return 'æ™šä¸Š (18:00-4:59)'
    df['time_period'] = df['hour'].apply(get_time_period)
    period_counts = df['time_period'].value_counts().reset_index()
    period_counts.columns = ['æ—¶é—´æ®µ', 'æé—®æ•°é‡']
    # è‡ªç„¶è¯­è¨€åˆ†æ
    most_active_period = period_counts.iloc[0]['æ—¶é—´æ®µ']
    hour_counts = df['hour'].value_counts().sort_index()
    peak_hour = hour_counts.idxmax()
    analysis_text = f"""
       ### ğŸ“ æ—¶é—´åˆ†æç»“è®ºï¼š
       - **æœ€æ´»è·ƒæ—¶æ®µ**ï¼š{most_active_period}ï¼ˆå å…¨å¤©æé—®é‡çš„{period_counts.iloc[0]['æé—®æ•°é‡'] / len(df) * 100:.1f}%ï¼‰
       - **æé—®é«˜å³°æ—¶åˆ»**ï¼š{peak_hour}:00 å·¦å³
       - **å­¦ä¹ æ—¶é—´åˆ†å¸ƒ**ï¼š{'å‡åŒ€' if hour_counts.max() / hour_counts.min() < 2 else 'é›†ä¸­'}

       ### ğŸš€ ä¸ªæ€§åŒ–å»ºè®®ï¼š
       1. åœ¨{peak_hour - 1}-{peak_hour + 1}ç‚¹çš„é«˜æ•ˆæ—¶æ®µè¿›è¡Œéš¾ç‚¹å­¦ä¹ 
       2. åˆ©ç”¨{most_active_period}è¿›è¡ŒçŸ¥è¯†å¤ç›˜
       3. åœ¨ä½æ´»è·ƒæ—¶æ®µå®‰æ’é¢„ä¹ æ€§å­¦ä¹ 
       """
    fig_pie = px.pie(
        period_counts,
        names='æ—¶é—´æ®µ',
        values='æé—®æ•°é‡',
        title='æé—®æ—¶é—´æ®µåˆ†å¸ƒ',
        color='æ—¶é—´æ®µ',
        color_discrete_sequence=px.colors.qualitative.Pastel
    )
    fig_pie.update_traces(textinfo='percent+label', pull=[0.05, 0, 0])

    hour_counts = df['hour'].value_counts().sort_index().reset_index()
    hour_counts.columns = ['å°æ—¶', 'æé—®æ•°é‡']
    fig_hour = px.bar(
        hour_counts,
        x='å°æ—¶',
        y='æé—®æ•°é‡',
        title='å„å°æ—¶æé—®æ•°é‡åˆ†å¸ƒ',
        labels={'å°æ—¶': 'æ—¶é—´ (å°æ—¶)', 'æé—®æ•°é‡': 'æé—®æ•°é‡'},
        color='å°æ—¶',
        color_continuous_scale='Viridis'
    )
    fig_hour.add_vrect(x0=5, x1=12, fillcolor="green", opacity=0.1, layer="below", line_width=0, annotation_text="ä¸Šåˆ")
    fig_hour.add_vrect(x0=12, x1=18, fillcolor="blue", opacity=0.1, layer="below", line_width=0, annotation_text="ä¸‹åˆ")
    fig_hour.add_vrect(x0=18, x1=24, fillcolor="purple", opacity=0.1, layer="below", line_width=0, annotation_text="æ™šä¸Š")
    fig_hour.add_vrect(x0=0, x1=5, fillcolor="purple", opacity=0.1, layer="below", line_width=0)

    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(fig_pie, use_container_width=True)
    with col2:
        st.plotly_chart(fig_hour, use_container_width=True)
    st.markdown(analysis_text)
# ---------------------------
# 3. çŸ¥è¯†ç‚¹å…±ç°åˆ†ææ¨¡å—
# ---------------------------
def build_cooccurrence_matrix(data):
    """
       æ„å»ºçŸ¥è¯†ç‚¹å…±ç°çŸ©é˜µ

       å‚æ•°:
           data: list[list] - äºŒç»´çŸ¥è¯†ç‚¹åˆ—è¡¨

       è¿”å›:
           tuple: (å…±ç°çŸ©é˜µDataFrame, çŸ¥è¯†ç‚¹åˆ—è¡¨)

       ç¤ºä¾‹:
           è¾“å…¥: [["A","B"], ["B","C"]]
           è¾“å‡º:
               DataFrame:
                   A  B  C
                 A 1  1  0
                 B 1  2  1
                 C 0  1  1
               ["A","B","C"]
       """
    all_knowledge_points = sorted(set([kp for record in data for kp in record]))
    cooccurrence_matrix = pd.DataFrame(0, index=all_knowledge_points, columns=all_knowledge_points)
    for record in data:
        valid_kps = [kp for kp in record if kp]
        for i, kp1 in enumerate(valid_kps):
            for kp2 in valid_kps[i:]:
                cooccurrence_matrix.loc[kp1, kp2] += 1
                if kp1 != kp2:
                    cooccurrence_matrix.loc[kp2, kp1] += 1
    return cooccurrence_matrix, all_knowledge_points


def analyze_knowledge_cooccurrence(data:list):
    """
       çŸ¥è¯†ç‚¹å…±ç°ç»¼åˆåˆ†æ

       å‚æ•°:
           data: list[list] - äºŒç»´çŸ¥è¯†ç‚¹åˆ—è¡¨

       åŠŸèƒ½:
           1. æ„å»ºå…±ç°çŸ©é˜µ
           2. è¯†åˆ«é«˜é¢‘å…±ç°å¯¹
           3. æ£€æµ‹çŸ¥è¯†ç¤¾åŒº
           4. å¯è§†åŒ–å…±ç°çƒ­åŠ›å›¾
           5. ç”Ÿæˆæ•™å­¦å»ºè®®
       """
    cooccurrence_matrix, all_kps = build_cooccurrence_matrix(data)

    # è‡ªç„¶è¯­è¨€æè¿°åˆ†æ
    cooccurrence_pairs = []
    for i, kp1 in enumerate(all_kps):
        for j, kp2 in enumerate(all_kps):
            if i < j and cooccurrence_matrix.loc[kp1, kp2] > 0:
                cooccurrence_pairs.append((kp1, kp2, cooccurrence_matrix.loc[kp1, kp2]))

    top_pairs = sorted(cooccurrence_pairs, key=lambda x: x[2], reverse=True)[:5]

    # æ„å»ºçŸ¥è¯†ç½‘ç»œè¿›è¡Œç¤¾åŒºæ£€æµ‹
    G = nx.Graph()
    for kp in all_kps:
        G.add_node(kp)
    for kp1, kp2, count in cooccurrence_pairs:
        if count >= 2:  # ç­›é€‰æ˜¾è‘—å…±ç°å…³ç³»
            G.add_edge(kp1, kp2, weight=count)

    communities = nx.algorithms.community.greedy_modularity_communities(G)

    # ç”Ÿæˆè‡ªç„¶è¯­è¨€åˆ†æ
    analysis_text = "### ğŸ“ å…±ç°åˆ†æç»“è®º\n"

    # é«˜é¢‘ç»„åˆåˆ†æ
    analysis_text += "#### é«˜é¢‘çŸ¥è¯†ç»„åˆï¼š\n"
    for pair in top_pairs:
        analysis_text += f"- **{pair[0]}** å’Œ **{pair[1]}** å…±åŒå‡ºç° {pair[2]} æ¬¡ï¼ˆå»ºè®®åŠ å¼ºç»„åˆç»ƒä¹ ï¼‰\n"

    # çŸ¥è¯†ç¾¤è½åˆ†æ
    analysis_text += "\n#### çŸ¥è¯†æ¨¡å—è¯†åˆ«ï¼š\n"
    for i, comm in enumerate(communities[:3]):  # æ˜¾ç¤ºå‰3ä¸ªä¸»è¦ç¤¾åŒº
        analysis_text += f"\n**æ¨¡å—{i + 1}**ï¼š{', '.join(list(comm)[:5])}{'ç­‰' if len(comm) > 5 else ''}\n"

    # è–„å¼±ç¯èŠ‚è¯†åˆ«
    degree_centrality = nx.degree_centrality(G)
    low_degree_kps = sorted(degree_centrality.items(), key=lambda x: x[1])[:3]
    analysis_text += "\n#### éœ€å…³æ³¨çŸ¥è¯†ç‚¹ï¼š\n"
    for kp, score in low_degree_kps:
        analysis_text += f"- **{kp}**ï¼ˆå…³è”åº¦è¾ƒä½ï¼Œå»ºè®®åŠ å¼ºä¸å…¶ä»–çŸ¥è¯†çš„è”ç³»ï¼‰\n"

    # æ•™å­¦å»ºè®®
    analysis_text += "\n#### ğŸš€ å­¦ä¹ å»ºè®®ï¼š\n"
    analysis_text += "1. ä¼˜å…ˆæŒæ¡é«˜é¢‘ç»„åˆä¸­çš„æ ¸å¿ƒçŸ¥è¯†\n"
    analysis_text += "2. æŒ‰çŸ¥è¯†æ¨¡å—è¿›è¡Œç³»ç»ŸåŒ–å¤ä¹ \n"
    analysis_text += "3. ä¸ºè–„å¼±çŸ¥è¯†ç‚¹è®¾è®¡ä¸“é¡¹ç»ƒä¹ \n"

    col1, col2 = st.columns([1, 2])
    with col1:
        st.subheader("ğŸ“Š çŸ¥è¯†ç‚¹å…±ç°é¢‘ç‡è¡¨")
        if cooccurrence_pairs:
            df_cooccurrence = pd.DataFrame(cooccurrence_pairs, columns=['çŸ¥è¯†ç‚¹1', 'çŸ¥è¯†ç‚¹2', 'å…±ç°é¢‘æ¬¡'])
            st.dataframe(df_cooccurrence.sort_values('å…±ç°é¢‘æ¬¡', ascending=False),
                         height=400,
                         use_container_width=True)

    with col2:
        st.subheader("ğŸ”¥ çŸ¥è¯†ç‚¹å…±ç°çƒ­åŠ›å›¾")
        fig = px.imshow(cooccurrence_matrix, text_auto=True, aspect="auto")
        st.plotly_chart(fig, use_container_width=True)

    st.markdown(analysis_text)

def build_knowledge_network(data, freq, min_cooccurrence=1, max_nodes=20):
    cooccurrence_matrix, all_kps = build_cooccurrence_matrix(data)
    top_kps = [kp for kp, count in freq.most_common(max_nodes)]
    subset_matrix = cooccurrence_matrix.loc[top_kps, top_kps]
    G = nx.Graph()
    for kp in top_kps:
        G.add_node(kp, frequency=freq[kp])
    for i, kp1 in enumerate(top_kps):
        for j, kp2 in enumerate(top_kps):
            if i < j:
                weight = subset_matrix.loc[kp1, kp2]
                if weight >= min_cooccurrence:
                    G.add_edge(kp1, kp2, weight=weight)
    edge_x = []
    edge_y = []
    pos = nx.spring_layout(G, seed=42)
    for edge in G.edges(data=True):
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=1, color='#888'),
        hoverinfo='none',
        mode='lines')
    node_x = []
    node_y = []
    node_text = []
    node_sizes = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_sizes.append(G.nodes[node]['frequency'])
        node_text.append(f"{node}<br>é¢‘ç‡: {G.nodes[node]['frequency']}")
    if node_sizes:
        max_size = max(node_sizes)
        node_sizes = [20 + (size / max_size) * 50 for size in node_sizes]
    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        text=list(G.nodes()),
        hovertext=node_text,
        hoverinfo='text',
        marker=dict(
            showscale=True,
            colorscale='YlGnBu',
            size=node_sizes,
            color=[G.nodes[node]['frequency'] for node in G.nodes()],
            colorbar=dict(
                thickness=15,
                title='çŸ¥è¯†ç‚¹é¢‘ç‡',
                xanchor='left',
                title_side='right'
            ),
            line=dict(width=2)
        )
    )
    fig = go.Figure(data=[edge_trace, node_trace],
                    layout=go.Layout(
                        title='çŸ¥è¯†ç‚¹å…±ç°ç½‘ç»œ',
                        titlefont=dict(size=16),
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20, l=5, r=5, t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=600
                    ))
    st.plotly_chart(fig, use_container_width=True)
    st.markdown("### ç½‘ç»œåˆ†æç»“æœ")
    if len(G.nodes()) > 0:
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        most_central_kp = max(degree_centrality.items(), key=lambda x: x[1])[0]
        most_betweenness_kp = max(betweenness_centrality.items(), key=lambda x: x[1])[0]
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ç½‘ç»œä¸­çš„çŸ¥è¯†ç‚¹æ•°é‡", len(G.nodes()))
            st.metric("ç½‘ç»œä¸­çš„è¿æ¥å…³ç³»æ•°é‡", len(G.edges()))
            st.metric("ç½‘ç»œå¯†åº¦", round(nx.density(G), 3))
        with col2:
            st.metric("æœ€æ ¸å¿ƒçŸ¥è¯†ç‚¹(åº¦ä¸­å¿ƒæ€§)", most_central_kp)
            st.metric("æœ€é‡è¦æ¡¥æ¥çŸ¥è¯†ç‚¹(ä¸­ä»‹ä¸­å¿ƒæ€§)", most_betweenness_kp)
            if len(G.nodes()) >= 3:
                communities = nx.community.greedy_modularity_communities(G)
                st.metric("çŸ¥è¯†ç‚¹ç¤¾åŒºæ•°é‡", len(communities))
    else:
        st.info("ç½‘ç»œä¸­æ²¡æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹è¿›è¡Œåˆ†æ")
# ---------------------------
# 4. å› æœæ¨æ–­ä¸çŸ¥è¯†å›¾è°±æ¨¡å—
# ---------------------------

def preprocess_causal_data(data, max_nodes=50):
    """é¢„å¤„ç†å› æœåˆ†ææ•°æ®ï¼ˆå¢å¼ºå¥å£®æ€§ï¼‰"""
    try:
        # æ•°æ®æ¸…æ´—
        clean_data = [
            [kp.strip() for kp in record if kp.strip()]
            for record in data
            if isinstance(record, (list, tuple))
        ]
        clean_data = [record for record in clean_data if record]

        # é¢‘ç‡ç»Ÿè®¡
        freq = Counter(kp for record in clean_data for kp in record)
        top_kps = [kp for kp, _ in freq.most_common(max_nodes)]

        if not top_kps:
            st.error("æ— æœ‰æ•ˆçŸ¥è¯†ç‚¹è¿›è¡Œåˆ†æ")
            return None, None

        # åˆ›å»ºå¸ƒå°”å‹ç‰¹å¾çŸ©é˜µ
        df = pd.DataFrame(False, columns=top_kps, index=range(len(clean_data)))
        for i, record in enumerate(clean_data):
            df.loc[i] = [kp in record for kp in top_kps]

        # å¼ºåˆ¶åˆ—é¡ºåºä¸€è‡´æ€§
        df = df.reindex(columns=top_kps)
        return df.astype(bool), top_kps
    except Exception as e:
        st.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
        return None, None


@st.cache_data
def causal_discovery(_df, alpha=0.01, method='pearson'):
    """ä½¿ç”¨PCç®—æ³•è¿›è¡Œå› æœå‘ç°"""
    """
     å› æœçŸ¥è¯†å›¾è°±æ„å»º

     å‚æ•°:
         df: é¢„å¤„ç†åçš„å¸ƒå°”å‹DataFrameï¼Œåˆ—è¡¨ç¤ºçŸ¥è¯†ç‚¹

     è¿”å›:
         nx.DiGraph - å› æœæœ‰å‘å›¾

     ç®—æ³•:
         ä½¿ç”¨PCç®—æ³•è¿›è¡Œå› æœå‘ç°:
         1. æ„å»ºæ— å‘éª¨æ¶
         2. å®šå‘Vå‹ç»“æ„
         3. å®šå‘å‰©ä½™è¾¹
     """
    est = PC(_df)
    model = est.estimate(variant="stable",
                         ci_test="chi_square",
                         alpha=alpha,
                         return_type="dag")

    # ç¡®ä¿æ¨¡å‹èŠ‚ç‚¹åç§°ä¸åˆ—åä¸€è‡´
    model.nodes = _df.columns.tolist()
    return model


def build_causal_knowledge_graph(model, feature_names):
    """æ„å»ºå› æœçŸ¥è¯†å›¾è°±ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
    try:
        G = nx.DiGraph()

        # éªŒè¯èŠ‚ç‚¹ä¸€è‡´æ€§
        model_nodes = model.nodes  # æ­£ç¡®è®¿é—®èŠ‚ç‚¹å±æ€§ï¼ˆéæ–¹æ³•è°ƒç”¨ï¼‰
        if not set(model_nodes) == set(feature_names):
            missing = set(feature_names) - set(model_nodes)
            extra = set(model_nodes) - set(feature_names)
            st.error(f"èŠ‚ç‚¹ä¸åŒ¹é…:\nç¼ºå¤±èŠ‚ç‚¹: {missing}\né¢å¤–èŠ‚ç‚¹: {extra}")
            return None

        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        G.add_nodes_from(model_nodes)
        G.add_edges_from(model.edges)

        return G
    except AttributeError as e:
        st.error(f"æ¨¡å‹ç»“æ„å¼‚å¸¸: {str(e)}")
        return None
    except Exception as e:
        st.error(f"å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
        return None


def plot_causal_graph(G):
    """äº¤äº’å¼å¯è§†åŒ–å› æœå›¾è°±"""
    pos = nx.spring_layout(G, seed=42)

    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=1.5, color='#888'),
        hoverinfo='none',
        mode='lines')

    node_x = []
    node_y = []
    node_text = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)

    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        text=node_text,
        textposition="top center",
        hoverinfo='text',
        marker=dict(
            size=20,
            color='lightblue',
            line=dict(width=2, color='darkblue')))

    fig = go.Figure(data=[edge_trace, node_trace],
                    layout=go.Layout(
                        title='å› æœçŸ¥è¯†å›¾è°±',
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20, l=5, r=5, t=40),
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=600
                    ))
    st.plotly_chart(fig, use_container_width=True)


def analyze_causal_relationships(data):
    """å› æœåˆ†æä¸»å‡½æ•°"""
    st.header("ğŸ§  å› æœçŸ¥è¯†å›¾è°±åˆ†æ")

    with st.expander("ğŸ” åˆ†ææ–¹æ³•è¯´æ˜", expanded=True):
        st.markdown("""
        ### å› æœå‘ç°æµç¨‹ï¼š
        1. **ç‰¹å¾é€‰æ‹©**ï¼šé€‰å–é«˜é¢‘çŸ¥è¯†ç‚¹ï¼ˆTop30ï¼‰
        2. **æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ**ï¼šä½¿ç”¨å¡æ–¹æ£€éªŒï¼ˆÎ±=0.01ï¼‰
        3. **éª¨æ¶å­¦ä¹ **ï¼šæ„å»ºæ— å‘å› æœéª¨æ¶
        4. **æ–¹å‘ç¡®å®š**ï¼šåŸºäºæ—¶åºæ•°æ®å’ŒVå‹ç»“æ„
        5. **å›¾è°±æ„å»º**ï¼šç”Ÿæˆæœ‰å‘æ— ç¯å›¾(DAG)

        ### æ•™å­¦è§£è¯»æŒ‡å—ï¼š
        - **ç®­å¤´æ–¹å‘**ï¼šè¡¨ç¤ºå¯èƒ½çš„å› æœå…³ç³»ï¼ˆAâ†’B è¡¨ç¤ºAå½±å“Bçš„æŒæ¡ï¼‰
        - **æ¢çº½èŠ‚ç‚¹**ï¼šå¤šä¸ªå…¥åº¦çš„çŸ¥è¯†ç‚¹å¯èƒ½éœ€è¦å‰ç½®å¼ºåŒ–
        - **å­¤ç«‹èŠ‚ç‚¹**ï¼šå¯èƒ½éœ€è¦å•ç‹¬æ•™å­¦æ¨¡å—
        - **é•¿è·¯å¾„**ï¼šæç¤ºæ ¸å¿ƒçŸ¥è¯†é“¾æ¡
        """)

    # å‚æ•°è®¾ç½®
    col1, col2 = st.columns(2)
    with col1:
        max_nodes = st.slider("æœ€å¤§åˆ†æçŸ¥è¯†ç‚¹æ•°", 10, 50, 30, key="causal_max_nodes")
    with col2:
        alpha = st.slider("æ˜¾è‘—æ€§æ°´å¹³Î±", 0.001, 0.1, 0.01, step=0.005)

    # å› æœåˆ†æ
    with st.spinner('æ­£åœ¨è¿›è¡Œå› æœå‘ç°...'):
        df_causal, features = preprocess_causal_data(data, max_nodes)
        model = causal_discovery(df_causal, alpha=alpha)
        G = build_causal_knowledge_graph(model, features)

    # å¯è§†åŒ–
    plot_causal_graph(G)

    # ç»“æ„åˆ†æ
    st.subheader("ğŸ“Œ å…³é”®ç»“æ„åˆ†æ")
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())

    causal_chains = []
    for node in G.nodes():
        if in_degrees[node] == 0 and out_degrees[node] > 1:
            causal_chains.append(node)

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ€»çŸ¥è¯†ç‚¹æ•°", len(G.nodes()))
        st.metric("æºå¤´çŸ¥è¯†ç‚¹", len([n for n, d in in_degrees.items() if d == 0]))
    with col2:
        st.metric("å¹³å‡å› æœé“¾é•¿åº¦",
                  round(np.mean([len(nx.dag_longest_path(G))]), 1))
        st.metric("å…³é”®æ¢çº½ç‚¹", len(causal_chains))
    with col3:
        st.metric("æœ€å¤§å…¥åº¦", max(in_degrees.values()))
        st.metric("æœ€å¤§å‡ºåº¦", max(out_degrees.values()))

    if causal_chains:
        st.markdown(f"**ä¸»è¦å› æœé“¾èµ·ç‚¹**: {', '.join(causal_chains[:3])}")
        # æ–°å¢è‡ªç„¶è¯­è¨€åˆ†æ
    if G and len(G.nodes) > 0:
        in_degrees = dict(G.in_degree())
        out_degrees = dict(G.out_degree())

        # å…³é”®èŠ‚ç‚¹è¯†åˆ«
        source_nodes = [n for n, d in in_degrees.items() if d == 0]
        hub_nodes = [n for n, d in out_degrees.items() if d >= 3]

        longest_path = nx.dag_longest_path(G)[:3]  # å–æœ€é•¿å› æœé“¾çš„å‰3ä¸ª
        analysis_text = f"""
                   ### ğŸ“ å› æœåˆ†æç»“è®ºï¼š
                   - **çŸ¥è¯†åŸºç¡€èŠ‚ç‚¹**ï¼š{', '.join(source_nodes[:3])}
                   - **æ ¸å¿ƒæ¢çº½èŠ‚ç‚¹**ï¼š{', '.join(hub_nodes[:3])}
                   - **æœ€é•¿å› æœé“¾**ï¼š{'â†’'.join(longest_path)}ï¼ˆæ·±åº¦ {len(nx.dag_longest_path(G))} çº§ï¼‰

                   ### ğŸš€ å­¦ä¹ è·¯å¾„å»ºè®®ï¼š
                   1. ä¼˜å…ˆæŒæ¡åŸºç¡€èŠ‚ç‚¹ï¼š{source_nodes[0] if source_nodes else ""}
                   2. é‡ç‚¹çªç ´æ¢çº½èŠ‚ç‚¹ï¼š{hub_nodes[0] if hub_nodes else ""}
                   3. æŒ‰å› æœé“¾é¡ºåºå­¦ä¹ ï¼š{' -> '.join(longest_path + [""] * (3 - len(longest_path)))}
                   """

        st.markdown(analysis_text)

# ---------------------------
# 5.å­¦ä¹ è¡Œä¸ºåˆ†ææ¨¡å—
# ---------------------------

def analyze_learning_sessions(df):
    """
    å­¦ä¹ ä¼šè¯åˆ†æ

    å‚æ•°:
        df: åŒ…å«timestampåˆ—çš„DataFrame

    è¿”å›:
        tuple: (åŸå§‹DataFrame, ä¼šè¯ç»Ÿè®¡DataFrame)

    åŠŸèƒ½:
        1. æ ¹æ®30åˆ†é’Ÿæ— æ´»åŠ¨åˆ’åˆ†ä¼šè¯
        2. ç»Ÿè®¡ä¼šè¯æ—¶é•¿ã€æé—®æ•°ç­‰æŒ‡æ ‡
        3. åˆ†æä¼šè¯æ—¶é—´åˆ†å¸ƒæ¨¡å¼
    """
    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼æ­£ç¡®
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.sort_values('timestamp')

    # å®šä¹‰ä¼šè¯é—´éš”é˜ˆå€¼ï¼ˆ30åˆ†é’Ÿæ— æ´»åŠ¨è§†ä¸ºæ–°ä¼šè¯ï¼‰
    SESSION_THRESHOLD = pd.Timedelta(minutes=30)

    # è®¡ç®—æ—¶é—´å·®å¹¶æ ‡è®°ä¼šè¯
    df['time_diff'] = df['timestamp'].diff()
    df['new_session'] = (df['time_diff'] > SESSION_THRESHOLD) | (df['time_diff'].isna())
    df['session_id'] = df['new_session'].cumsum()

    # è®¡ç®—æ¯ä¸ªä¼šè¯çš„ç»Ÿè®¡ä¿¡æ¯
    session_stats = df.groupby('session_id').agg(
        session_start=('timestamp', 'min'),
        session_end=('timestamp', 'max'),
        session_duration=('timestamp', lambda x: (x.max() - x.min()).total_seconds() / 60),
        questions_count=('timestamp', 'count'),
        unique_knowledge_points=('knowledge_points', lambda x: len(set(kp for sublist in x for kp in sublist)))
    ).reset_index()

    # æ·»åŠ ä¼šè¯æ—¶æ®µåˆ†ç±»
    def get_time_category(time):
        hour = time.hour
        if 5 <= hour < 9:
            return 'æ—©æ™¨ (5:00-8:59)'
        elif 9 <= hour < 12:
            return 'ä¸Šåˆ (9:00-11:59)'
        elif 12 <= hour < 14:
            return 'ä¸­åˆ (12:00-13:59)'
        elif 14 <= hour < 18:
            return 'ä¸‹åˆ (14:00-17:59)'
        elif 18 <= hour < 22:
            return 'æ™šä¸Š (18:00-21:59)'
        else:
            return 'æ·±å¤œ (22:00-4:59)'

    session_stats['time_category'] = session_stats['session_start'].apply(get_time_category)

    # å¯è§†åŒ–ä¼šè¯åˆ†å¸ƒ
    st.subheader("ğŸ“Š å­¦ä¹ ä¼šè¯æ—¶æ®µåˆ†å¸ƒ")
    time_category_counts = session_stats['time_category'].value_counts().reset_index()
    time_category_counts.columns = ['æ—¶æ®µ', 'ä¼šè¯æ•°é‡']

    # å¯¹æ—¶æ®µæŒ‰ä¸€å¤©çš„æ—¶é—´é¡ºåºæ’åº
    time_order = ['æ—©æ™¨ (5:00-8:59)', 'ä¸Šåˆ (9:00-11:59)', 'ä¸­åˆ (12:00-13:59)',
                  'ä¸‹åˆ (14:00-17:59)', 'æ™šä¸Š (18:00-21:59)', 'æ·±å¤œ (22:00-4:59)']
    time_category_counts['æ—¶æ®µ'] = pd.Categorical(time_category_counts['æ—¶æ®µ'], categories=time_order, ordered=True)
    time_category_counts = time_category_counts.sort_values('æ—¶æ®µ')

    fig = px.bar(
        time_category_counts,
        x='æ—¶æ®µ',
        y='ä¼šè¯æ•°é‡',
        color='æ—¶æ®µ',
        title='å­¦ä¹ ä¼šè¯æ—¶æ®µåˆ†å¸ƒ',
        labels={'æ—¶æ®µ': 'æ—¶é—´æ®µ', 'ä¼šè¯æ•°é‡': 'ä¼šè¯æ•°é‡'}
    )
    return df, session_stats


def analyze_knowledge_learning_curve(df):
    """
    åˆ†æçŸ¥è¯†ç‚¹å­¦ä¹ æ›²çº¿ï¼Œè·Ÿè¸ªå­¦ä¹ è¿›åº¦å’Œå…³æ³¨ç‚¹å˜åŒ–

    å‚æ•°:
        df: åŒ…å«timestampå’Œknowledge_pointsçš„DataFrame
    """
    st.subheader("ğŸ“ˆ çŸ¥è¯†ç‚¹å…³æ³¨åº¦å˜åŒ–è¶‹åŠ¿")

    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼åŒ–å¹¶æŒ‰å‘¨èšåˆ
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['week'] = df['timestamp'].dt.strftime('%Y-%U')

    # å±•å¼€çŸ¥è¯†ç‚¹åˆ—è¡¨å¹¶è®¡ç®—æ¯å‘¨çŸ¥è¯†ç‚¹é¢‘ç‡
    weekly_kp_data = []
    for week, group in df.groupby('week'):
        week_kps = []
        for kps in group['knowledge_points']:
            if isinstance(kps, list):
                week_kps.extend(kps)
            else:
                week_kps.extend([kp.strip() for kp in kps.split(',')])

        # è·å–æœ¬å‘¨Top5çŸ¥è¯†ç‚¹
        top_kps = Counter(week_kps).most_common(5)
        for kp, count in top_kps:
            weekly_kp_data.append({
                'å‘¨': week,
                'çŸ¥è¯†ç‚¹': kp,
                'æé—®æ¬¡æ•°': count
            })

    if not weekly_kp_data:
        st.warning("æ²¡æœ‰è¶³å¤Ÿçš„å‘¨æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return

    weekly_df = pd.DataFrame(weekly_kp_data)

    # è®¡ç®—æ‰€æœ‰å‘¨çš„Top10çŸ¥è¯†ç‚¹
    all_kps = [kp for kps in df['knowledge_points'] for kp in
               (kps if isinstance(kps, list) else [k.strip() for k in kps.split(',')])]
    top10_kps = [kp for kp, _ in Counter(all_kps).most_common(10)]

    # è¿‡æ»¤åªæ˜¾ç¤ºTop10çŸ¥è¯†ç‚¹çš„è¶‹åŠ¿
    top_kp_trends = weekly_df[weekly_df['çŸ¥è¯†ç‚¹'].isin(top10_kps)]

    if len(top_kp_trends) > 0:
        # åˆ›å»ºçŸ¥è¯†ç‚¹å­¦ä¹ è¶‹åŠ¿å›¾
        fig = px.line(
            top_kp_trends,
            x='å‘¨',
            y='æé—®æ¬¡æ•°',
            color='çŸ¥è¯†ç‚¹',
            markers=True,
            title='ä¸»è¦çŸ¥è¯†ç‚¹å…³æ³¨åº¦å‘¨è¶‹åŠ¿',
            labels={'å‘¨': 'å‘¨æ¬¡', 'æé—®æ¬¡æ•°': 'æé—®é¢‘æ¬¡'}
        )
        fig.update_layout(xaxis_title='å‘¨æ¬¡', yaxis_title='æé—®é¢‘æ¬¡', legend_title='çŸ¥è¯†ç‚¹')
        st.plotly_chart(fig, use_container_width=True)

        # çƒ­é—¨çŸ¥è¯†ç‚¹è½¬ç§»å›¾
        st.subheader("ğŸ”„ å­¦ä¹ ç„¦ç‚¹è½¬ç§»åˆ†æ")

        # ä¸ºæ¯å‘¨æ‰¾å‡ºæœ€çƒ­é—¨çŸ¥è¯†ç‚¹
        weekly_hot_kps = []
        for week, group in weekly_df.groupby('å‘¨'):
            top_kp = group.loc[group['æé—®æ¬¡æ•°'].idxmax()]
            weekly_hot_kps.append({
                'å‘¨æ¬¡': week,
                'çƒ­ç‚¹çŸ¥è¯†ç‚¹': top_kp['çŸ¥è¯†ç‚¹'],
                'æé—®æ¬¡æ•°': top_kp['æé—®æ¬¡æ•°']
            })

        hot_kps_df = pd.DataFrame(weekly_hot_kps)

        # åˆ›å»ºçƒ­é—¨çŸ¥è¯†ç‚¹è½¬ç§»è¡¨æ ¼
        st.dataframe(
            hot_kps_df,
            column_config={
                "å‘¨æ¬¡": st.column_config.TextColumn("å‘¨æ¬¡"),
                "çƒ­ç‚¹çŸ¥è¯†ç‚¹": st.column_config.TextColumn("çƒ­ç‚¹çŸ¥è¯†ç‚¹"),
                "æé—®æ¬¡æ•°": st.column_config.NumberColumn("æé—®æ¬¡æ•°", format="%d")
            },
            use_container_width=True,
            hide_index=True
        )

        # æå–å­¦ä¹ è¡Œä¸ºè§è§£
        if len(hot_kps_df) > 1:
            focus_changes = sum(1 for i in range(len(hot_kps_df) - 1)
                                if hot_kps_df.iloc[i]['çƒ­ç‚¹çŸ¥è¯†ç‚¹'] != hot_kps_df.iloc[i + 1]['çƒ­ç‚¹çŸ¥è¯†ç‚¹'])

            st.markdown(f"""
            ### ğŸ“ å­¦ä¹ è¡Œä¸ºåˆ†æï¼š

            - **å…³æ³¨ç‚¹ç¨³å®šæ€§**: {'è¾ƒä½' if focus_changes / len(hot_kps_df) > 0.5 else 'è¾ƒé«˜'}
            - **ç„¦ç‚¹è½¬ç§»é¢‘ç‡**: {focus_changes} æ¬¡è½¬ç§» / {len(hot_kps_df)} å‘¨
            - **å­¦ä¹ é£æ ¼æ¨æ–­**: {'å¯èƒ½åå‘äºå¹¿æ³›å­¦ä¹ å¤šä¸ªä¸»é¢˜' if focus_changes / len(hot_kps_df) > 0.5 else 'å¯èƒ½ä¸“æ³¨äºç³»ç»ŸæŒæ¡å°‘é‡ä¸»é¢˜'}
            """)


def analyze_learning_intensity(df):
    """
    åˆ†æå­¦ä¹ å¼ºåº¦å˜åŒ–å’Œå­¦ä¹ è§„å¾‹

    å‚æ•°:
        df: åŒ…å«timestampå’Œknowledge_pointsçš„DataFrame
    """
    st.subheader("ğŸ”¥ å­¦ä¹ å¼ºåº¦åˆ†æ")

    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼æ­£ç¡®
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # æŒ‰æ—¥æœŸç»Ÿè®¡æé—®é‡
    df['date'] = df['timestamp'].dt.date
    daily_counts = df.groupby('date').size().reset_index(name='æé—®æ•°é‡')
    daily_counts['date'] = pd.to_datetime(daily_counts['date'])
    daily_counts['æ˜ŸæœŸ'] = daily_counts['date'].dt.day_name()

    # æ·»åŠ æ˜ŸæœŸå‡ çš„æ’åº
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    weekday_map = {
        'Monday': 'å‘¨ä¸€', 'Tuesday': 'å‘¨äºŒ', 'Wednesday': 'å‘¨ä¸‰',
        'Thursday': 'å‘¨å››', 'Friday': 'å‘¨äº”', 'Saturday': 'å‘¨å…­', 'Sunday': 'å‘¨æ—¥'
    }
    daily_counts['æ˜ŸæœŸ'] = daily_counts['æ˜ŸæœŸ'].map(weekday_map)
    daily_counts['æ˜ŸæœŸåºå·'] = daily_counts['date'].dt.dayofweek

    # åˆ›å»ºå­¦ä¹ å¼ºåº¦è¶‹åŠ¿å›¾
    fig = px.line(
        daily_counts,
        x='date',
        y='æé—®æ•°é‡',
        title='æ¯æ—¥å­¦ä¹ å¼ºåº¦å˜åŒ–è¶‹åŠ¿',
        labels={'date': 'æ—¥æœŸ', 'æé—®æ•°é‡': 'æé—®æ•°é‡'}
    )
    fig.update_layout(xaxis_title='æ—¥æœŸ', yaxis_title='æé—®æ•°é‡')
    st.plotly_chart(fig, use_container_width=True)

    # æŒ‰æ˜ŸæœŸå‡ åˆ†ç»„æŸ¥çœ‹å­¦ä¹ ä¹ æƒ¯
    weekday_stats = daily_counts.groupby('æ˜ŸæœŸ').agg(
        å¹³å‡æé—®æ•°=('æé—®æ•°é‡', 'mean'),
        æœ€å¤§æé—®æ•°=('æé—®æ•°é‡', 'max'),
        æœ€å°æé—®æ•°=('æé—®æ•°é‡', 'min')
    ).reset_index()

    # ç¡®ä¿æŒ‰æ˜ŸæœŸå‡ æ’åº
    weekday_stats['æ’åº'] = weekday_stats['æ˜ŸæœŸ'].map({v: k for k, v in enumerate(map(weekday_map.get, weekday_order))})
    weekday_stats = weekday_stats.sort_values('æ’åº').drop('æ’åº', axis=1)

    # å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ğŸ“… æŒ‰æ˜ŸæœŸå‡ å­¦ä¹ å¼ºåº¦")
        fig = px.bar(
            weekday_stats,
            x='æ˜ŸæœŸ',
            y='å¹³å‡æé—®æ•°',
            color='æ˜ŸæœŸ',
            title='æ˜ŸæœŸå‡ å¹³å‡å­¦ä¹ å¼ºåº¦',
            text='å¹³å‡æé—®æ•°',
            labels={'æ˜ŸæœŸ': 'æ˜ŸæœŸ', 'å¹³å‡æé—®æ•°': 'å¹³å‡æé—®æ•°é‡'}
        )
        fig.update_layout(xaxis={'categoryorder': 'array', 'categoryarray': list(map(weekday_map.get, weekday_order))})
        fig.update_traces(texttemplate='%{text:.1f}', textposition='outside')
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # å·¥ä½œæ—¥vså‘¨æœ«
        daily_counts['æ˜¯å¦å‘¨æœ«'] = daily_counts['æ˜ŸæœŸ'].isin(['å‘¨å…­', 'å‘¨æ—¥']).map({True: 'å‘¨æœ«', False: 'å·¥ä½œæ—¥'})
        weekend_vs_weekday = daily_counts.groupby('æ˜¯å¦å‘¨æœ«')['æé—®æ•°é‡'].mean().reset_index()

        fig = px.pie(
            weekend_vs_weekday,
            values='æé—®æ•°é‡',
            names='æ˜¯å¦å‘¨æœ«',
            title='å·¥ä½œæ—¥ vs å‘¨æœ«å¹³å‡å­¦ä¹ å¼ºåº¦',
            color='æ˜¯å¦å‘¨æœ«',
            color_discrete_map={'å·¥ä½œæ—¥': '#3366CC', 'å‘¨æœ«': '#DC3912'},
            hole=0.4
        )
        fig.update_traces(textinfo='percent+label+value')
        st.plotly_chart(fig, use_container_width=True)

    # è®¡ç®—å­¦ä¹ è§„å¾‹æ€§æŒ‡æ ‡
    cv = daily_counts['æé—®æ•°é‡'].std() / daily_counts['æé—®æ•°é‡'].mean() if daily_counts['æé—®æ•°é‡'].mean() > 0 else 0
    weekend_ratio = weekend_vs_weekday[weekend_vs_weekday['æ˜¯å¦å‘¨æœ«'] == 'å‘¨æœ«']['æé—®æ•°é‡'].values[0] / \
                    weekend_vs_weekday[weekend_vs_weekday['æ˜¯å¦å‘¨æœ«'] == 'å·¥ä½œæ—¥']['æé—®æ•°é‡'].values[0] \
        if 'å·¥ä½œæ—¥' in weekend_vs_weekday['æ˜¯å¦å‘¨æœ«'].values and \
           weekend_vs_weekday[weekend_vs_weekday['æ˜¯å¦å‘¨æœ«'] == 'å·¥ä½œæ—¥']['æé—®æ•°é‡'].values[0] > 0 else 0

    most_active_day = weekday_stats.loc[weekday_stats['å¹³å‡æé—®æ•°'].idxmax()]['æ˜ŸæœŸ']

    st.markdown(f"""
    ### ğŸ’¡ å­¦ä¹ è§„å¾‹åˆ†æï¼š

    - **å­¦ä¹ è§„å¾‹æ€§æŒ‡æ•°**: {(1 - min(cv, 1)) * 100:.1f}% (è¶Šé«˜è¡¨ç¤ºå­¦ä¹ å¼ºåº¦è¶Šç¨³å®š)
    - **å‘¨æœ«/å·¥ä½œæ—¥æ¯”ä¾‹**: {weekend_ratio:.2f} (>1è¡¨ç¤ºå‘¨æœ«å­¦ä¹ æ›´å¤šï¼Œ<1è¡¨ç¤ºå·¥ä½œæ—¥å­¦ä¹ æ›´å¤š)
    - **æœ€æ´»è·ƒå­¦ä¹ æ—¥**: {most_active_day}
    - **å»ºè®®**: {"å»ºè®®æ›´å‡è¡¡åˆ†é…å­¦ä¹ æ—¶é—´ï¼Œä¿æŒç¨³å®šçš„å­¦ä¹ èŠ‚å¥" if cv > 0.5 else "å½“å‰å­¦ä¹ å¼ºåº¦åˆ†é…è¾ƒä¸ºå‡è¡¡ï¼Œå»ºè®®ä¿æŒ"}
    """)


# ---------------------------
# 6. ä¸ªæ€§åŒ–åé¦ˆä¸é¢„æµ‹æ¨¡å—
# ---------------------------

def create_learning_profile(df, data):
    """
    åˆ›å»ºå­¦ç”Ÿä¸ªæ€§åŒ–å­¦ä¹ ç”»åƒ

    å‚æ•°:
        df: åŒ…å«timestampå’Œknowledge_pointsçš„DataFrame
        data: knowledge_pointsæ•°æ®åˆ—è¡¨
    """
    """
    ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ ç”»åƒ

    å‚æ•°:
        df: åŒ…å«æ—¶é—´æˆ³å’ŒçŸ¥è¯†ç‚¹çš„åŸå§‹DataFrame
        data: äºŒç»´çŸ¥è¯†ç‚¹åˆ—è¡¨

    åŠŸèƒ½:
        1. åˆ†æå­¦ä¹ å¼ºåº¦æ¨¡å¼
        2. è¯„ä¼°çŸ¥è¯†ç‚¹æŒæ¡åº¦
        3. ç”Ÿæˆå¯è§†åŒ–å­¦ä¹ æŠ¥å‘Š
        4. æä¾›ä¸ªæ€§åŒ–å»ºè®®
    """
    st.header("ğŸ§  å­¦ç”Ÿä¸ªæ€§åŒ–å­¦ä¹ ç”»åƒ")

    with st.expander("ğŸ“Š ä»€ä¹ˆæ˜¯å­¦ä¹ ç”»åƒï¼Ÿ", expanded=True):
        st.markdown("""
        ### å­¦ä¹ ç”»åƒå¸®åŠ©æ‚¨äº†è§£ï¼š
        - å­¦ç”Ÿçš„**å­¦ä¹ è¡Œä¸ºæ¨¡å¼**å’Œ**çŸ¥è¯†æŒæ¡çŠ¶å†µ**
        - å­¦ä¹ è¿‡ç¨‹ä¸­çš„**å¼ºé¡¹**å’Œ**è–„å¼±ç¯èŠ‚**
        - åŸºäºæ•°æ®åˆ†æçš„**ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®**

        *å­¦ä¹ ç”»åƒåŸºäºå­¦ç”Ÿçš„æé—®è®°å½•è‡ªåŠ¨ç”Ÿæˆï¼Œå¯ä¸ºæ•™å­¦æä¾›å‚è€ƒã€‚*
        """)

    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    # æå–æ‰€æœ‰å”¯ä¸€çŸ¥è¯†ç‚¹
    all_kps = []
    for kps in data:
        if isinstance(kps, list):
            all_kps.extend(kps)
        else:
            all_kps.extend([k.strip() for k in k.split(',')])

    kp_freq = Counter(all_kps)

    # 1. å­¦ä¹ å¼ºåº¦æ¦‚è§ˆ
    total_questions = len(df)
    active_days = df['timestamp'].dt.date.nunique()
    avg_daily = total_questions / active_days if active_days > 0 else 0

    # 2. è®¡ç®—å­¦ä¹ æ—¶æ®µåå¥½
    df['hour'] = df['timestamp'].dt.hour

    def get_period(hour):
        if 5 <= hour < 12:
            return 'ä¸Šåˆ'
        elif 12 <= hour < 18:
            return 'ä¸‹åˆ'
        else:
            return 'æ™šä¸Š'

    df['period'] = df['hour'].apply(get_period)
    period_counts = df['period'].value_counts()
    preferred_period = period_counts.idxmax() if not period_counts.empty else "æ— æ˜æ˜¾åå¥½"

    # 3. çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µè¯„ä¼°
    top_kps = kp_freq.most_common(5)
    bottom_kps = kp_freq.most_common()[:-6:-1] if len(kp_freq) > 5 else []

    # 4. å­¦ä¹ è¿è´¯æ€§è¯„ä¼°
    df = df.sort_values('timestamp')
    df['next_timestamp'] = df['timestamp'].shift(-1)
    df['time_diff_hours'] = (df['next_timestamp'] - df['timestamp']).dt.total_seconds() / 3600
    avg_interval = df['time_diff_hours'].mean()

    # è¿ç»­æ€§æŒ‡æ ‡ (é—´éš”å°äº8å°æ—¶çš„å æ¯”)
    continuity = (df['time_diff_hours'] < 8).mean() if len(df) > 1 else 0

    # 5. åˆ›å»ºå­¦ä¹ ç”»åƒå¡ç‰‡
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ğŸ“Š å­¦ä¹ æ´»è·ƒåº¦")
        metric_cols = st.columns(3)
        metric_cols[0].metric("æ€»æé—®æ•°", f"{total_questions}")
        metric_cols[1].metric("æ´»è·ƒå¤©æ•°", f"{active_days}å¤©")
        metric_cols[2].metric("æ—¥å‡æé—®", f"{avg_daily:.1f}ä¸ª")

        # å­¦ä¹ é£æ ¼é›·è¾¾å›¾
        # è®¡ç®—å„ç»´åº¦æŒ‡æ ‡ (0-1ä¹‹é—´)
        regularity = 1 - min(df['time_diff_hours'].std() / max(df['time_diff_hours'].mean(), 1), 1) if len(
            df) > 1 else 0
        topic_focus = 1 - min(len(kp_freq) / max(len(all_kps), 1), 1)
        persistence = continuity
        time_management = 1 - (df['hour'].isin([23, 0, 1, 2, 3, 4])).mean()
        variety = min(len(kp_freq) / 10, 1) if len(kp_freq) > 0 else 0

        radar_data = pd.DataFrame({
            'ç»´åº¦': ['ä¸“æ³¨åº¦', 'å­¦ä¹ æŒç»­æ€§', 'æ—¶é—´ç®¡ç†', 'çŸ¥è¯†å¤šæ ·æ€§'],
            'å¾—åˆ†': [topic_focus, persistence, time_management, variety]
        })

        fig = px.line_polar(
            radar_data,
            r='å¾—åˆ†',
            theta='ç»´åº¦',
            line_close=True,
            range_r=[0, 1],
            title="å­¦ä¹ é£æ ¼é›·è¾¾å›¾"
        )
        fig.update_traces(fill='toself')
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.markdown("### ğŸ§© çŸ¥è¯†å…³æ³¨ç‚¹")

        # åˆ›å»ºçƒ­é—¨çŸ¥è¯†ç‚¹æ¡å½¢å›¾
        if top_kps:
            top_kps_df = pd.DataFrame(top_kps, columns=['çŸ¥è¯†ç‚¹', 'æé—®æ¬¡æ•°'])
            fig = px.bar(
                top_kps_df,
                y='çŸ¥è¯†ç‚¹',
                x='æé—®æ¬¡æ•°',
                orientation='h',
                title='çƒ­é—¨å…³æ³¨çŸ¥è¯†ç‚¹',
                color='æé—®æ¬¡æ•°',
                color_continuous_scale='Viridis'
            )
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)

        # å­¦ä¹ æ—¶æ®µåå¥½
        st.markdown(f"### â° å­¦ä¹ æ—¶æ®µåå¥½: **{preferred_period}**")
        if not period_counts.empty:
            fig = px.pie(
                names=period_counts.index,
                values=period_counts.values,
                title='å­¦ä¹ æ—¶æ®µåˆ†å¸ƒ',
                hole=0.4
            )
            fig.update_traces(textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)

    # 6. å­¦ä¹ å»ºè®®
    st.markdown("### ğŸ“ ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®")

    # ç”Ÿæˆå­¦ä¹ å»ºè®®
    recommendations = []

    # æ ¹æ®è§„å¾‹æ€§ç”Ÿæˆå»ºè®®
    if regularity < 0.4:
        recommendations.append("ğŸ“† **æé«˜å­¦ä¹ è§„å¾‹æ€§**ï¼šå»ºè®®åˆ¶å®šå›ºå®šçš„å­¦ä¹ è®¡åˆ’ï¼Œä¿æŒæ¯æ—¥å­¦ä¹ ä¹ æƒ¯ã€‚")

    # æ ¹æ®æ—¶æ®µåå¥½ç”Ÿæˆå»ºè®®
    if preferred_period == 'æ™šä¸Š':
        recommendations.append("â° **ä¼˜åŒ–å­¦ä¹ æ—¶é—´**ï¼šå°è¯•å°†éƒ¨åˆ†å­¦ä¹ æ—¶é—´è°ƒæ•´åˆ°ç™½å¤©ï¼Œå¯èƒ½æœ‰åŠ©äºæé«˜å­¦ä¹ æ•ˆç‡ã€‚")

    # æ ¹æ®çŸ¥è¯†ç‚¹åˆ†å¸ƒç”Ÿæˆå»ºè®®
    if topic_focus < 0.3:
        recommendations.append("ğŸ¯ **æé«˜å­¦ä¹ ä¸“æ³¨åº¦**ï¼šå½“å‰å­¦ä¹ ä¸»é¢˜è¾ƒä¸ºåˆ†æ•£ï¼Œå»ºè®®é˜¶æ®µæ€§åœ°ä¸“æ³¨äºç‰¹å®šçŸ¥è¯†é¢†åŸŸï¼Œæ·±å…¥å­¦ä¹ ã€‚")

    # æ ¹æ®æŒç»­æ€§ç”Ÿæˆå»ºè®®
    if persistence < 0.4:
        recommendations.append("âš¡ **å¢å¼ºå­¦ä¹ æŒç»­æ€§**ï¼šæ‚¨çš„å­¦ä¹ é—´éš”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®é‡‡ç”¨æ›´è¿è´¯çš„å­¦ä¹ æ–¹å¼ï¼Œå‡å°‘é•¿æ—¶é—´ä¸­æ–­ã€‚")

    # æ ¹æ®æ—¶é—´ç®¡ç†ç”Ÿæˆå»ºè®®
    if time_management < 0.7:
        recommendations.append("ğŸŒ™ **æ”¹å–„æ—¶é—´ç®¡ç†**ï¼šé¿å…åœ¨æ·±å¤œå­¦ä¹ ï¼Œä¿æŒè‰¯å¥½çš„ä½œæ¯ä¹ æƒ¯æœ‰åŠ©äºæé«˜å­¦ä¹ æ•ˆç‡ã€‚")

    # æ ¹æ®çƒ­é—¨çŸ¥è¯†ç‚¹ç”Ÿæˆå»ºè®®
    if top_kps:
        difficult_topic = top_kps[0][0]
        recommendations.append(f"ğŸ“š **é‡ç‚¹çŸ¥è¯†å·©å›º**ï¼š'{difficult_topic}'æ˜¯æ‚¨æé—®æœ€å¤šçš„çŸ¥è¯†ç‚¹ï¼Œå»ºè®®è¿›è¡Œç³»ç»Ÿæ€§å¤ä¹ å’Œç»ƒä¹ ã€‚")

    # å¦‚æœå»ºè®®å¾ˆå°‘ï¼Œæ·»åŠ ä¸€æ¡é€šç”¨å»ºè®®
    if len(recommendations) < 2:
        recommendations.append("ğŸŒŸ **ä¿æŒè‰¯å¥½ä¹ æƒ¯**ï¼šæ‚¨çš„å­¦ä¹ æ¨¡å¼æ•´ä½“è‰¯å¥½ï¼Œå»ºè®®ä¿æŒå½“å‰çš„å­¦ä¹ èŠ‚å¥å’Œæ–¹æ³•ã€‚")

    # æ˜¾ç¤ºå»ºè®®
    for rec in recommendations:
        st.markdown(rec)

    # 7. é¢„æµ‹æœªæ¥å­¦ä¹ è¶‹åŠ¿
    if len(df) >= 10:  # åªæœ‰æ•°æ®è¶³å¤Ÿæ—¶æ‰è¿›è¡Œé¢„æµ‹
        st.markdown("### ğŸ”® å­¦ä¹ è¶‹åŠ¿é¢„æµ‹")

        # ç®€å•çš„çº¿æ€§é¢„æµ‹
        df['date'] = df['timestamp'].dt.date
        daily_counts = df.groupby('date').size().reset_index(name='count')
        daily_counts['date'] = pd.to_datetime(daily_counts['date'])
        daily_counts['day_num'] = range(len(daily_counts))

        # ä½¿ç”¨è¿‡å»çš„æ•°æ®é¢„æµ‹æœªæ¥7å¤©
        if len(daily_counts) >= 5:
            try:
                from sklearn.linear_model import LinearRegression

                X = daily_counts['day_num'].values.reshape(-1, 1)
                y = daily_counts['count'].values

                model = LinearRegression()
                model.fit(X, y)

                # é¢„æµ‹æœªæ¥7å¤©
                future_days = np.array(range(len(daily_counts), len(daily_counts) + 7)).reshape(-1, 1)
                predictions = model.predict(future_days)

                # åˆ›å»ºé¢„æµ‹æ•°æ®æ¡†
                last_date = daily_counts['date'].max()
                future_dates = [last_date + pd.Timedelta(days=i + 1) for i in range(7)]
                future_df = pd.DataFrame({
                    'date': future_dates,
                    'count': predictions,
                    'type': 'é¢„æµ‹'
                })

                # åˆå¹¶å†å²å’Œé¢„æµ‹æ•°æ®
                daily_counts['type'] = 'å†å²'
                plot_data = pd.concat([
                    daily_counts[['date', 'count', 'type']],
                    future_df
                ])

                # ç»˜åˆ¶é¢„æµ‹è¶‹åŠ¿å›¾
                fig = px.line(
                    plot_data,
                    x='date',
                    y='count',
                    color='type',
                    title='å­¦ä¹ å¼ºåº¦è¶‹åŠ¿é¢„æµ‹ (æœªæ¥7å¤©)',
                    labels={'date': 'æ—¥æœŸ', 'count': 'æé—®æ•°é‡', 'type': 'æ•°æ®ç±»å‹'}
                )
                fig.update_layout(xaxis_title='æ—¥æœŸ', yaxis_title='é¢„è®¡æé—®æ•°é‡')
                st.plotly_chart(fig, use_container_width=True)

                # è¶‹åŠ¿è§£è¯»
                trend = "ä¸Šå‡" if model.coef_[0] > 0.1 else "ä¸‹é™" if model.coef_[0] < -0.1 else "ç¨³å®š"
                st.markdown(f"""
                ### ğŸ“ˆ è¶‹åŠ¿è§£è¯»ï¼š

                - **å­¦ä¹ å¼ºåº¦è¶‹åŠ¿**: {trend}
                - **é¢„è®¡å˜åŒ–ç‡**: {model.coef_[0]:.2f} æé—®/å¤©
                - **æœªæ¥7å¤©å¹³å‡æé—®é‡**: {predictions.mean():.1f} æé—®/å¤©
                """)

            except Exception as e:
                st.warning(f"æ— æ³•ç”Ÿæˆé¢„æµ‹: {str(e)}")
        else:
            st.info("æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯é çš„è¶‹åŠ¿é¢„æµ‹ã€‚å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ã€‚")


# ---------------------------
# 6.é«˜çº§æ—¶åºåˆ†ææ¨¡å—
# ---------------------------
def advanced_time_series_analysis(df):
    """
    é’ˆå¯¹èŠå¤©/æé—®è®°å½•è¿›è¡Œæ›´æ·±åº¦çš„æ—¶åºåˆ†æï¼ŒåŒ…å«ï¼š
    1. æ´»åŠ¨å³°å€¼ä¸å‘¨æœŸæ€§æ£€æµ‹
    2. å¯¹è¯ä¸»é¢˜æ¼”å˜ä¸å…³è”æ€§ï¼ˆéœ€å­˜åœ¨ 'text' å­—æ®µï¼‰
    3. çŸ¥è¯†ç‚¹æ—¶åºè¶‹åŠ¿ä¸è¡Œä¸ºè½¨è¿¹åˆ†æ
    4. ç®€å•çŸ¥è¯†è¿½è¸ªæ¨¡å‹ï¼ˆåŸºäºè¿‘æœŸä¸å…¨æœŸæé—®é¢‘æ¬¡å¯¹æ¯”ï¼‰
    è¦æ±‚æ•°æ®ä¸­è‡³å°‘åŒ…å« 'timestamp'ã€'knowledge_points'ï¼ˆåˆ—è¡¨æˆ–å­—ç¬¦ä¸²æ ¼å¼ï¼‰ä»¥åŠå¯é€‰çš„ 'text' å­—æ®µ
    """
    # 1. æ•°æ®é¢„å¤„ç†
    try:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    except Exception as e:
        st.error(f"æ—¶é—´æˆ³è½¬æ¢é”™è¯¯: {e}")
        return
    df = df.sort_values('timestamp')
    df['date'] = df['timestamp'].dt.date

    st.header("ğŸ” æ·±åº¦æ—¶åºåˆ†æ")
    # ---------------------------
    # 4. çŸ¥è¯†ç‚¹æ—¶åºè¶‹åŠ¿ä¸è¡Œä¸ºè½¨è¿¹
    # ---------------------------
    st.subheader("3. çŸ¥è¯†ç‚¹æ—¶åºè¶‹åŠ¿ä¸è¡Œä¸ºè½¨è¿¹")
    # å°† knowledge_points è½¬ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆå¦‚æœéåˆ—è¡¨ï¼Œåˆ™æŒ‰é€—å·åˆ†å‰²ï¼‰
    if isinstance(df['knowledge_points'].iloc[0], list):
        kp_series = df['knowledge_points']
    else:
        kp_series = df['knowledge_points'].apply(lambda x: [kp.strip() for kp in str(x).split(',')])
    # æ„å»ºæ¯ä¸ªçŸ¥è¯†ç‚¹çš„æ¯æ—¥å‡ºç°é¢‘æ¬¡
    daily_kp_records = []
    for idx, row in df.iterrows():
        date_val = row['date']
        kps = row['knowledge_points'] if isinstance(row['knowledge_points'], list) else [kp.strip() for kp in str(
            row['knowledge_points']).split(',')]
        for kp in kps:
            daily_kp_records.append({'date': date_val, 'knowledge_point': kp})
    kp_df = pd.DataFrame(daily_kp_records)
    if kp_df.empty:
        st.info("çŸ¥è¯†ç‚¹è®°å½•ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ—¶åºè¶‹åŠ¿åˆ†æã€‚")
    else:
        # æ„å»ºæ•°æ®é€è§†è¡¨ï¼ˆæ—¥æœŸ x çŸ¥è¯†ç‚¹å‡ºç°æ¬¡æ•°ï¼‰
        kp_pivot = kp_df.groupby(['date', 'knowledge_point']).size().reset_index(name='count')
        kp_table = kp_pivot.pivot(index='date', columns='knowledge_point', values='count').fillna(0)
        st.line_chart(kp_table)  # ç®€å•æŠ˜çº¿å›¾å±•ç¤ºå„çŸ¥è¯†ç‚¹çš„æ—¥è¶‹åŠ¿

        # æ£€æµ‹è¿‘æœŸæ˜¯å¦æœ‰çŸ¥è¯†ç‚¹é¢‘æ¬¡çªç„¶å¢åŠ 
        st.markdown("**è¿‘æœŸè¡Œä¸ºè½¨è¿¹æ£€æµ‹**ï¼š")
        # è®¾å®šåŸºçº¿æœŸï¼ˆå…¨æœŸå¹³å‡ï¼‰å’Œè¿‘æœŸï¼ˆä¾‹å¦‚æœ€è¿‘ 7 å¤©ï¼‰
        baseline_period = kp_table.index.min(), kp_table.index.max()
        recent_period = kp_table.index.max() - pd.Timedelta(days=7), kp_table.index.max()
        baseline_avg = kp_table.mean()
        recent_avg = kp_table.loc[kp_table.index >= recent_period[0]].mean()
        sudden_increase = (recent_avg - baseline_avg) / (baseline_avg + 1e-6)  # é˜²æ­¢é™¤0
        increase_threshold = st.number_input("æ£€æµ‹é˜ˆå€¼ï¼ˆ%ï¼‰", min_value=0.0, value=0.5)
        flagged = sudden_increase[sudden_increase > increase_threshold]
        if not flagged.empty:
            st.markdown("ä¸‹åˆ—çŸ¥è¯†ç‚¹è¿‘æœŸæé—®é¢‘æ¬¡æ˜¾è‘—å¢åŠ ï¼š")
            for kp, inc in flagged.items():
                st.markdown(f"- **{kp}**ï¼šå¢åŠ æ¯”ä¾‹ {inc:.2f}")
        else:
            st.info("æ— æ˜æ˜¾é¢‘æ¬¡çªç„¶å¢åŠ çš„çŸ¥è¯†ç‚¹ã€‚")

    # ---------------------------
    # 5. ç®€å•çŸ¥è¯†è¿½è¸ªæ¨¡å‹
    # ---------------------------
    st.subheader("4. ç®€å•çŸ¥è¯†è¿½è¸ªæ¨¡å‹")
    st.markdown("""
    å‡è®¾å­¦ç”Ÿå¯¹æŸçŸ¥è¯†ç‚¹çš„åå¤æé—®è¡¨æ˜æŒæ¡ä¸è¶³ï¼Œåˆ©ç”¨å…¨æœŸä¸è¿‘æœŸï¼ˆä¾‹å¦‚æœ€è¿‘ 7 å¤©ï¼‰çš„æé—®é¢‘æ¬¡å¯¹æ¯”ï¼Œç»™å‡ºä¸€ä¸ªç®€å•çš„æŒæ¡åº¦ä¼°è®¡ï¼š
    \næŒæ¡åº¦ = 1 - (è¿‘æœŸé¢‘æ¬¡ / (å…¨æœŸå¹³å‡é¢‘æ¬¡ + 1e-6))
    \næŒæ¡åº¦èŒƒå›´ä¸º 0~1ï¼Œå€¼è¶Šä½è¡¨ç¤ºæŒæ¡å¯èƒ½è¶Šå·®ã€‚
    """)
    # è®¡ç®—å…¨æœŸå¹³å‡é¢‘æ¬¡å’Œæœ€è¿‘7å¤©å¹³å‡é¢‘æ¬¡
    mastery_df = pd.DataFrame({'çŸ¥è¯†ç‚¹': kp_table.columns,
                               'å…¨æœŸå‡å€¼': kp_table.mean().values})
    recent_data = kp_table.loc[kp_table.index >= recent_period[0]]
    if not recent_data.empty:
        mastery_df['è¿‘æœŸå‡å€¼'] = recent_data.mean().values
    else:
        mastery_df['è¿‘æœŸå‡å€¼'] = mastery_df['å…¨æœŸå‡å€¼']
    mastery_df['æŒæ¡åº¦'] = 1 - (mastery_df['è¿‘æœŸå‡å€¼'] / (mastery_df['å…¨æœŸå‡å€¼'] + 1e-6))
    mastery_df['æŒæ¡åº¦'] = mastery_df['æŒæ¡åº¦'].clip(0, 1)
    mastery_df = mastery_df.sort_values('æŒæ¡åº¦')
    st.dataframe(mastery_df[['çŸ¥è¯†ç‚¹', 'å…¨æœŸå‡å€¼', 'è¿‘æœŸå‡å€¼', 'æŒæ¡åº¦']], use_container_width=True)

    st.markdown("### ç»¼åˆç»“è®º")
    st.markdown(""" 
    - **çŸ¥è¯†ç‚¹æ—¶åºè¶‹åŠ¿ä¸è¡Œä¸ºè½¨è¿¹** å¯å¸®åŠ©è¯†åˆ«è¿‘æœŸé¢‘æ¬¡çªç„¶å¢åŠ çš„çŸ¥è¯†ç‚¹ï¼Œä»è€ŒåŠæ—¶å¹²é¢„ã€‚  
    - **çŸ¥è¯†è¿½è¸ªæ¨¡å‹** æä¾›äº†åŸºäºæé—®è¡Œä¸ºçš„æŒæ¡åº¦ä¼°è®¡ï¼Œä¸ºä¸ªæ€§åŒ–æ•™å­¦æä¾›æ•°æ®æ”¯æ’‘ã€‚
    """)
# ---------------------------
# 7.æ—¶é—´é©±åŠ¨çš„è®°å¿†æŒä¹…æ€§åˆ†æ
# ---------------------------
def analyze_memory_persistence(df):
    """
    åŸºäºæé—®æ—¶é—´é—´éš”çš„é—å¿˜æ›²çº¿å»ºæ¨¡
    åˆ›æ–°æ–¹æ³•ï¼šé€šè¿‡æé—®é—´éš”æ¨¡å¼æ¨å¯¼è®°å¿†å¼ºåº¦
    """
    """
        åŸºäºæ—¶é—´é—´éš”çš„è®°å¿†å¼ºåº¦åˆ†æ

        å‚æ•°:
            df: åŒ…å«timestampå’Œknowledge_pointsçš„DataFrame

        ç®—æ³•:
            1. è®¡ç®—ç›¸é‚»æé—®æ—¶é—´é—´éš”
            2. ä½¿ç”¨æŒ‡æ•°è¡°å‡æ¨¡å‹ä¼°ç®—è®°å¿†å¼ºåº¦
            3. ç”Ÿæˆä¸ªæ€§åŒ–é—å¿˜æ›²çº¿
            4. æ¨èæœ€ä½³å¤ä¹ æ—¶é—´ç‚¹

        å…¬å¼:
            è®°å¿†å¼ºåº¦ S = -Î”t / ln(R)
            å…¶ä¸­Rä¸ºè®°å¿†ä¿ç•™ç‡é˜ˆå€¼(é»˜è®¤0.7)
        """
    st.header("ğŸ§  è®°å¿†æŒä¹…æ€§åˆ†æï¼ˆæ—¶é—´é©±åŠ¨ç‰ˆï¼‰")

    # ================= æ•°æ®é¢„å¤„ç† =================
    # å±•å¼€çŸ¥è¯†ç‚¹å¹¶æ¸…æ´—
    df_exp = df.explode('knowledge_points')
    df_exp['knowledge_points'] = df_exp['knowledge_points'].str.strip()
    df_exp = df_exp[df_exp['knowledge_points'] != '']

    # è½¬æ¢æ—¶é—´æˆ³å¹¶æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„
    df_exp['timestamp'] = pd.to_datetime(df_exp['timestamp'])
    df_exp = df_exp.sort_values(['knowledge_points', 'timestamp'])

    # ================= æ ¸å¿ƒç®—æ³• =================
    def calculate_memory_strength(timestamps):
        """
        åŸºäºæ—¶é—´é—´éš”è®¡ç®—è®°å¿†å¼ºåº¦ç³»æ•°S
        åˆ›æ–°å‡è®¾ï¼šå­¦ç”Ÿä¸»åŠ¨æé—®é—´éš”åæ˜ è®°å¿†å¼ºåº¦
        é—´éš”è¶Šé•¿â†’å‡è®¾è®°å¿†å¼ºåº¦è¶Šé«˜
        """
        if len(timestamps) < 2:
            return None

        # è®¡ç®—æ—¶é—´é—´éš”ï¼ˆå¤©ï¼‰
        intervals = []
        prev_ts = timestamps[0]
        for ts in timestamps[1:]:
            delta = (ts - prev_ts).total_seconds() / 86400
            intervals.append(delta)
            prev_ts = ts

        # åŠ¨æ€æƒé‡è®¡ç®—ï¼ˆè¿‘æœŸé—´éš”æƒé‡æ›´é«˜ï¼‰
        weights = np.linspace(0.5, 1.5, len(intervals))

        # åˆ›æ–°å…¬å¼ï¼šå‡è®¾ç†æƒ³ä¿ç•™ç‡R=0.7æ—¶å­¦ç”Ÿä¼šæé—®
        # æ¨å¯¼å…¬å¼ï¼šS = -Î”t / ln(R)
        R = 0.7  # ç»éªŒé˜ˆå€¼
        weighted_S = np.average([-t / np.log(R) for t in intervals], weights=weights)

        return max(weighted_S, 0.1)  # é˜²æ­¢è´Ÿå€¼

    # ================= æ‰§è¡Œåˆ†æ =================
    analysis_results = []
    industry_avg = 5.0  # è¡Œä¸šåŸºå‡†å€¼

    for kp, group in df_exp.groupby('knowledge_points'):
        timestamps = group['timestamp'].tolist()
        if len(timestamps) < 2:
            continue

        # è®¡ç®—è®°å¿†å¼ºåº¦ç³»æ•°
        S = calculate_memory_strength(timestamps)
        if S is None:
            continue

        # ç”Ÿæˆæ¨èé—´éš”ï¼ˆä¿ç•™ç‡ç›®æ ‡80%ï¼‰
        optimal_interval = -S * np.log(0.8)

        analysis_results.append({
            "çŸ¥è¯†ç‚¹": kp,
            "æé—®æ¬¡æ•°": len(timestamps),
            "é¦–æ¬¡æé—®": timestamps[0].strftime("%Y-%m-%d"),
            "æœ€åæé—®": timestamps[-1].strftime("%Y-%m-%d"),
            "è®°å¿†å¼ºåº¦(S)": S,
            "æ¨èé—´éš”": round(optimal_interval, 1),
            "è¡Œä¸šå¯¹æ¯”": "é«˜äº" if S > industry_avg else "ä½äº"
        })

    # ================= å¯è§†åŒ–å±•ç¤º =================
    if analysis_results:
        df_analysis = pd.DataFrame(analysis_results)

        # æ˜¾ç¤ºæ ¸å¿ƒæŒ‡æ ‡
        st.subheader("ğŸ“Š è®°å¿†å¼ºåº¦åˆ†ææ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        avg_S = df_analysis['è®°å¿†å¼ºåº¦(S)'].mean()
        col1.metric("å¹³å‡è®°å¿†å¼ºåº¦", f"{avg_S:.1f}", f"{avg_S - industry_avg:+.1f} vs è¡Œä¸š")
        col2.metric("æœ€å¼ºçŸ¥è¯†ç‚¹",
                    df_analysis.loc[df_analysis['è®°å¿†å¼ºåº¦(S)'].idxmax()]['çŸ¥è¯†ç‚¹'],
                    f"S={df_analysis['è®°å¿†å¼ºåº¦(S)'].max():.1f}")
        col3.metric("éœ€å…³æ³¨çŸ¥è¯†ç‚¹",
                    df_analysis.loc[df_analysis['è®°å¿†å¼ºåº¦(S)'].idxmin()]['çŸ¥è¯†ç‚¹'],
                    f"S={df_analysis['è®°å¿†å¼ºåº¦(S)'].min():.1f}")

        # äº¤äº’å¼åˆ†æ
        selected_kp = st.selectbox("é€‰æ‹©çŸ¥è¯†ç‚¹", df_analysis['çŸ¥è¯†ç‚¹'].tolist())
        kp_data = df_analysis[df_analysis['çŸ¥è¯†ç‚¹'] == selected_kp].iloc[0]

        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        st.markdown(f"""
        ### ğŸ“ {selected_kp} åˆ†æç»“æœ
        - **è®°å¿†å¼ºåº¦ç³»æ•°**: {kp_data['è®°å¿†å¼ºåº¦(S)']:.1f} ({kp_data['è¡Œä¸šå¯¹æ¯”']}è¡Œä¸šå¹³å‡)
        - **å†å²æé—®æ¬¡æ•°**: {kp_data['æé—®æ¬¡æ•°']} æ¬¡
        - **å­¦ä¹ å‘¨æœŸ**: {kp_data['é¦–æ¬¡æé—®']} è‡³ {kp_data['æœ€åæé—®']}
        - **æ¨èå¤ä¹ é—´éš”**: {kp_data['æ¨èé—´éš”']} å¤©
        """)

        # ç»˜åˆ¶é—å¿˜æ›²çº¿
        st.subheader("ğŸ”® ä¸ªæ€§åŒ–é—å¿˜æ›²çº¿")
        days = np.linspace(0, 30, 100)
        retention = np.exp(-days / kp_data['è®°å¿†å¼ºåº¦(S)'])
        fig = px.area(
            x=days, y=retention,
            labels={'x': 'è·ä¸Šæ¬¡å­¦ä¹ å¤©æ•°', 'y': 'é¢„è®¡è®°å¿†ä¿ç•™ç‡'},
            title=f"'{selected_kp}' é—å¿˜æ›²çº¿ (S={kp_data['è®°å¿†å¼ºåº¦(S)']:.1f})"
        )
        fig.add_vline(x=kp_data['æ¨èé—´éš”'], line_dash="dot",
                      annotation_text=f"æ¨èå¤ä¹ æ—¶é—´")
        fig.add_hrect(y0=0.7, y1=0.7, line_width=0,
                      annotation_text="æé—®é˜ˆå€¼", opacity=0.2)
        st.plotly_chart(fig, use_container_width=True)

        # ç”Ÿæˆå¤ä¹ è®¡åˆ’
        st.subheader("ğŸ“… æ™ºèƒ½å¤ä¹ è®¡åˆ’")
        last_study = pd.to_datetime(kp_data['æœ€åæé—®'])
        next_review = last_study + pd.Timedelta(days=kp_data['æ¨èé—´éš”'])
        review_plan = [
            ("ç«‹å³å¤ä¹ ", last_study.strftime("%m-%d"), "å·©å›ºè®°å¿†"),
            ("é¦–æ¬¡å¤ä¹ ", next_review.strftime("%m-%d"), "æœ€ä½³è®°å¿†ç‚¹"),
            ("äºŒæ¬¡å¤ä¹ ", (next_review + pd.Timedelta(days=kp_data['æ¨èé—´éš”'] * 1.5)).strftime("%m-%d"), "é•¿æœŸå·©å›º")
        ]

        cols = st.columns(3)
        for i, (title, date, desc) in enumerate(review_plan):
            cols[i].metric(title, date, desc)

    else:
        st.warning("""
        ## æ— æ³•ç”Ÿæˆåˆ†æçš„å¯èƒ½åŸå› ï¼š
        1. æ²¡æœ‰çŸ¥è¯†ç‚¹å…·æœ‰â‰¥2æ¬¡æé—®è®°å½•
        2. æ—¶é—´é—´éš”è¿‡çŸ­ï¼ˆ<1å°æ—¶ï¼‰
        3. æ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡®
        """)

# ---------------------------
# ä¸»ç¨‹åº
# ---------------------------
def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°

    åŠŸèƒ½:
        1. æ–‡ä»¶ä¸Šä¼ ä¸è§£æ
        2. è·¯ç”±åˆ°å„åˆ†ææ¨¡å—
        3. ç•Œé¢å¸ƒå±€ç®¡ç†
    """
    st.set_page_config(page_title="å­¦ç”ŸçŸ¥è¯†ç‚¹åˆ†æå·¥å…·", page_icon="ğŸ“Š", layout="wide")
    st.title("ğŸ“š å­¦ç”Ÿæé—®çŸ¥è¯†ç‚¹åˆ†æå·¥å…·")
    st.markdown("---")
    st.header("1ï¸âƒ£ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼  CSV æˆ– JSON æ–‡ä»¶ï¼ˆéœ€åŒ…å« knowledge_points å’Œ timestamp åˆ—ï¼Œé€‰å¡« text åˆ—ï¼‰",
                                     type=["csv", "json"])

    if uploaded_file is not None:
        try:
            file_extension = uploaded_file.name.split('.')[-1].lower()
            if file_extension == 'csv':
                df = pd.read_csv(uploaded_file)
            elif file_extension == 'json':
                data_json = json.load(uploaded_file)
                df = pd.DataFrame(data_json)
            else:
                st.error("è¯·ä¸Šä¼  CSV æˆ– JSON æ ¼å¼çš„æ•°æ®æ–‡ä»¶")
                st.stop()

            if 'knowledge_points' not in df.columns:
                st.error("æ•°æ®ä¸­ç¼ºå°‘ 'knowledge_points' åˆ—ï¼")
                st.stop()
            if 'timestamp' not in df.columns:
                st.warning("æ•°æ®ä¸­ç¼ºå°‘ 'timestamp' åˆ—ï¼Œéƒ¨åˆ†æ—¶åºåˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨ï¼")

            # åˆ¤æ–­ knowledge_points æ˜¯å¦å·²ç»ä¸ºåˆ—è¡¨æ ¼å¼
            if isinstance(df['knowledge_points'].iloc[0], list):
                data = df['knowledge_points'].tolist()
            else:
                data = df['knowledge_points'].apply(lambda x: [kp.strip() for kp in str(x).split(',')]).tolist()

            freq = knowledge_frequency_analysis(data)
            # åœ¨tabåˆ—è¡¨ä¸­æ·»åŠ æ–°çš„åˆ†æé¡µ
            tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8,tab9= st.tabs([
                "ğŸ“Š çŸ¥è¯†ç‚¹é¢‘ç‡åˆ†æ",
                "ğŸ”„ çŸ¥è¯†ç‚¹å…±ç°åˆ†æ",
                "â° æ—¶åºè¶‹åŠ¿åˆ†æ",
                "â±ï¸ æ—¶é—´åå¥½åˆ†æ",
                "ğŸ“ˆ æ¯æ—¥çŸ¥è¯†ç‚¹ç»„æˆ",
                "ğŸ§  å› æœçŸ¥è¯†å›¾è°±",
                "âœ¨ ä¸ªæ€§åŒ–åé¦ˆ",
                "ğŸ” æ·±åº¦æ—¶åºåˆ†æ",
                "ğŸ§  è®°å¿†æŒä¹…æ€§"
            ])

            with tab1:
                st.header("2ï¸âƒ£ çŸ¥è¯†ç‚¹é¢‘ç‡åˆ†æ")
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("ğŸ“‹ çŸ¥è¯†ç‚¹é¢‘ç‡è¡¨æ ¼")
                    plot_frequency_table(freq)
                with col2:
                    st.subheader("â˜ï¸ çŸ¥è¯†ç‚¹è¯äº‘å›¾")
                    plot_frequency_wordcloud_streamlit(freq)
                st.markdown("---")
                st.subheader("3ï¸âƒ£ çŸ¥è¯†ç‚¹é¢‘ç‡TOP10å›¾è¡¨")
                chart_type = st.radio("é€‰æ‹©å›¾è¡¨ç±»å‹ï¼š", options=["æŸ±çŠ¶å›¾", "é¥¼å›¾"], horizontal=True, key="chart_type")
                if chart_type == "æŸ±çŠ¶å›¾":
                    plot_top_frequency_bar(freq, top_n=10)
                else:
                    plot_top_frequency_pie(freq, top_n=10)

            with tab2:
                st.header("ğŸ”„ çŸ¥è¯†ç‚¹å…±ç°åˆ†æ")
                with st.expander("ğŸ“– åˆ†æè¯´æ˜ä¸è§£è¯»æŒ‡å—", expanded=True):
                    st.markdown("""
                       ### å¦‚ä½•è§£è¯»å…±ç°åˆ†æï¼Ÿ
                       1. **å…±ç°é¢‘ç‡è¡¨**ï¼šæ˜¾ç¤ºçŸ¥è¯†ç‚¹ä¸¤ä¸¤ç»„åˆçš„å‡ºç°æ¬¡æ•°ï¼Œé«˜é¢‘ç»„åˆæç¤ºæ•™å­¦ä¸­çš„å¸¸è§çŸ¥è¯†å…³è”
                       2. **çƒ­åŠ›å›¾**ï¼šé¢œè‰²è¶Šæ·±è¡¨ç¤ºå…±ç°é¢‘ç‡è¶Šé«˜ï¼Œå¯¹è§’çº¿æ˜¾ç¤ºå•ä¸ªçŸ¥è¯†ç‚¹å‡ºç°é¢‘æ¬¡
                       3. **ç½‘ç»œå›¾**ï¼š
                          - èŠ‚ç‚¹å¤§å°åæ˜ çŸ¥è¯†ç‚¹å‡ºç°é¢‘ç‡
                          - è¿çº¿ç²—ç»†è¡¨ç¤ºå…±ç°å¼ºåº¦
                          - ç´«è‰²èŠ‚ç‚¹è¡¨ç¤ºæ ¸å¿ƒæ¢çº½çŸ¥è¯†ç‚¹
                          - ç´§å¯†è¿æ¥çš„ç¾¤è½æç¤ºçŸ¥è¯†æ¨¡å—

                       ### æ•™å­¦åº”ç”¨ä»·å€¼ï¼š
                       âœ… å‘ç°é«˜é¢‘ç»„åˆ â†’ ä¼˜åŒ–è¯¾ç¨‹è®¾è®¡ä¸­çš„çŸ¥è¯†ç‚¹æ­é…  
                       âœ… è¯†åˆ«æ ¸å¿ƒèŠ‚ç‚¹ â†’ åŠ å¼ºé‡ç‚¹çŸ¥è¯†ç‚¹çš„æ•™å­¦  
                       âœ… å‘ç°çŸ¥è¯†ç¾¤è½ â†’ å»ºç«‹æ¨¡å—åŒ–æ•™å­¦ä½“ç³»  
                       âœ… å®šä½è–„å¼±ç¯èŠ‚ â†’ å‘ç°åº”åŠ å¼ºå…³è”çš„æ•™å­¦ç‚¹

                       *ç¤ºä¾‹ï¼šè‹¥"ä¸‰è§’å‡½æ•°"ä¸"å‘é‡"é«˜é¢‘å…±ç°ï¼Œå»ºè®®åœ¨æ•™å­¦ä¸­å¼ºåŒ–äºŒè€…çš„ç»¼åˆåº”ç”¨è®­ç»ƒ*
                       """)
                analyze_knowledge_cooccurrence(data)
                st.markdown("---")
                st.subheader("ğŸ•¸ï¸ çŸ¥è¯†ç‚¹å…±ç°ç½‘ç»œ")
                col1, col2 = st.columns(2)
                with col1:
                    min_cooccurrence = st.slider("æœ€å°å…±ç°é˜ˆå€¼", min_value=1, max_value=10, value=1,
                                                 help="åªæ˜¾ç¤ºå…±ç°æ¬¡æ•°å¤§äºç­‰äºæ­¤å€¼çš„çŸ¥è¯†ç‚¹å¯¹")
                with col2:
                    max_nodes = st.slider("æœ€å¤§èŠ‚ç‚¹æ•°é‡", min_value=5, max_value=30, value=15,
                                          help="é™åˆ¶ç½‘ç»œå›¾ä¸­æ˜¾ç¤ºçš„çŸ¥è¯†ç‚¹æ•°é‡")
                build_knowledge_network(data, freq, min_cooccurrence, max_nodes)

            with tab3:
                if 'timestamp' in df.columns:
                    st.header("â° æ—¶åºè¶‹åŠ¿åˆ†æ")
                    with st.expander("ğŸ” åˆ†æç»´åº¦è¯´æ˜", expanded=True):
                        st.markdown("""
                        ### æœ¬æ¨¡å—åˆ†æç»´åº¦åŒ…æ‹¬ï¼š
                        1. **å­¦ä¹ ä¼šè¯åˆ†æ**ï¼šè¯†åˆ«è¿ç»­å­¦ä¹ æ—¶æ®µå’Œé—´éš”
                        2. **å­¦ä¹ å¼ºåº¦åˆ†æ**ï¼šåˆ†ææ¯æ—¥/æ¯å‘¨å­¦ä¹ è§„å¾‹
                        3. **çŸ¥è¯†ç„¦ç‚¹è¿ç§»**ï¼šè·Ÿè¸ªçŸ¥è¯†ç‚¹å…³æ³¨åº¦å˜åŒ–
                        """)

                    df_enriched, session_stats = analyze_learning_sessions(df)
                    analyze_knowledge_learning_curve(df_enriched)
                    analyze_learning_intensity(df_enriched)

                else:
                    st.error("æ— æ³•è¿›è¡Œæ—¶åºåˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

            with tab4:
                if 'timestamp' in df.columns:
                    st.header("â±ï¸ å­¦ç”Ÿæé—®æ—¶é—´åå¥½åˆ†æ")
                    analyze_time_preference(df)
                else:
                    st.error("æ— æ³•è¿›è¡Œæ—¶é—´åˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

            with tab5:
                if 'timestamp' in df.columns:
                    st.header("ğŸ“ˆ æ¯æ—¥çŸ¥è¯†ç‚¹ç»„æˆåˆ†æ")
                    analyze_daily_knowledge_composition(df)
                else:
                    st.error("æ— æ³•è¿›è¡Œæ¯æ—¥åˆ†æï¼Œæ•°æ®ä¸­ç¼ºå°‘ timestamp åˆ—")

            with tab6:
                if 'timestamp' in df.columns:
                    analyze_causal_relationships(data)
                else:
                    st.error("éœ€è¦æ—¶é—´æˆ³æ•°æ®è¿›è¡Œå› æœæ—¶åºåˆ†æ")

            with tab7:
                st.header("âœ¨ ä¸ªæ€§åŒ–åé¦ˆ")
                create_learning_profile(df, data)
            with tab8:
                advanced_time_series_analysis(df)
            with tab9:  # æ–°å¢åˆ†ææ¨¡å—
                if 'timestamp' in df.columns:
                    analyze_memory_persistence(df.copy())
                else:
                    st.error("éœ€è¦æ—¶é—´æˆ³æ•°æ®è¿›è¡Œè®°å¿†æŒä¹…æ€§åˆ†æ")
        except Exception as e:
            st.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            st.exception(e)
    else:
        st.info("""
        ### ğŸ‘‹ ä½¿ç”¨è¯´æ˜
        1. è¯·å‡†å¤‡åŒ…å«å­¦ç”Ÿæé—®çŸ¥è¯†ç‚¹çš„æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒ CSV æˆ– JSON æ ¼å¼ã€‚
        2. æ•°æ®æ–‡ä»¶ä¸­å¿…é¡»åŒ…å«ä»¥ä¸‹é”®/åˆ—ï¼š
           - `knowledge_points`: å¦‚æœæ˜¯ CSV æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼›å¦‚æœæ˜¯ JSONï¼Œåˆ™åº”ä¸ºåˆ—è¡¨æ ¼å¼ã€‚
           - `timestamp`: æé—®æ—¶é—´ï¼Œæ ¼å¼å¦‚ "2023-01-01 10:30:00"
           - å¯é€‰ï¼š`text` å­—æ®µç”¨äºå¯¹è¯ä¸»é¢˜åˆ†æ
        3. ä¸Šä¼ æ–‡ä»¶åï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ†æå¹¶å±•ç¤ºå¤šç§å¯è§†åŒ–ç»“æœã€‚
        """)

if __name__ == "__main__":
    main()
