import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
import pandas as np
import streamlit as st
import pandas as pd
import json
from collections import Counter
from sklearn.metrics import pairwise_distances
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import OneHotEncoder
from langchain_community.llms import  Tongyi
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import networkx as nx
import plotly.graph_objects as go
import numpy as np
import pandas as pd
import time
plt.rcParams['font.sans-serif'] = ['SimHei']  # é€‚ç”¨äº Windows
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³åæ ‡è½´è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main {background-color: #f8f9fa;}
    .stMetricLabel {font-size: 0.9rem !important;}
    .stMetricValue {font-size: 1.2rem !important;}
    .cluster-box {border-radius:10px; padding:15px; margin:10px 0;}
    .tab-container {margin-top: 20px;}
</style>
""", unsafe_allow_html=True)


def validate_analysis_data(results):
    """æ•°æ®æ ¡éªŒé€»è¾‘"""
    required_keys = {
        'top_weak_points': (list, 3),
        'weak_distribution': (dict, 5),
        'study_period_dist': (dict, 3),
        'question_stats': (dict, ['mean', 'std']),
        'cluster_summary': (dict, ['total_clusters', 'cluster_dist']),
        'knowledge_network': (dict, ['top_combinations', 'network_density'])
    }

    for key, (typ, min_len) in required_keys.items():
        if key not in results:
            raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {key}")
        if not isinstance(results[key], typ):
            raise TypeError(f"{key} ç±»å‹é”™è¯¯ï¼Œåº”ä¸º {typ}")

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if isinstance(min_len, list):  # å­—å…¸é”®æ£€æŸ¥
            for k in min_len:
                if k not in results[key]:
                    raise ValueError(f"{key} ç¼ºå°‘å­å­—æ®µ: {k}")
        elif isinstance(min_len, int):  # æœ€å°é•¿åº¦æ£€æŸ¥
            if len(results[key]) < min_len:
                raise Warning(f"{key} æ•°æ®é‡ä¸è¶³ï¼Œå¯èƒ½å½±å“åˆ†ææ·±åº¦")

# -----------------------------
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# -----------------------------
@st.cache_data
def load_data(uploaded_file=None):
    """åŠ è½½æ•°æ®ï¼Œæ”¯æŒä¸Šä¼ æˆ–ä½¿ç”¨ç¤ºä¾‹æ•°æ®"""
    if uploaded_file:
        data = json.load(uploaded_file)
    else:
        sample_json = '''[
          {"name":"å¼ ä¸‰","weak_points":["ä»£æ•°","å‡ ä½•"],"study_period":"æ—©æ™¨","question_count":5},
          {"name":"æå››","weak_points":["ä»£æ•°","å¾®ç§¯åˆ†"],"study_period":"ä¸‹åˆ","question_count":3},
          {"name":"ç‹äº”","weak_points":["å‡ ä½•","ç»Ÿè®¡"],"study_period":"æ™šä¸Š","question_count":7},
          {"name":"èµµå…­","weak_points":["å¾®ç§¯åˆ†","ä»£æ•°"],"study_period":"æ—©æ™¨","question_count":4},
          {"name":"é™ˆä¸ƒ","weak_points":["ç»Ÿè®¡"],"study_period":"ä¸‹åˆ","question_count":6},
          {"name":"å‘¨å…«","weak_points":["ä»£æ•°","ç»Ÿè®¡"],"study_period":"æ™šä¸Š","question_count":8}
        ]'''
        data = json.loads(sample_json)
    return pd.DataFrame(data)


# -----------------------------
# è–„å¼±çŸ¥è¯†ç‚¹åˆ†ææ¨¡å—
# -----------------------------
def analyze_weak_points(df):
    st.header("ğŸ“š è–„å¼±çŸ¥è¯†ç‚¹åˆ†æ", divider="rainbow")

    # æ•°æ®é¢„å¤„ç†
    all_weak_points = [point for sublist in df['weak_points'] for point in sublist]
    weak_counter = Counter(all_weak_points)
    df_weak = pd.DataFrame(weak_counter.items(), columns=['çŸ¥è¯†ç‚¹', 'å‡ºç°æ¬¡æ•°']).sort_values('å‡ºç°æ¬¡æ•°',
                                                                                             ascending=False)

    # åªä¿ç•™å‰äº”çŸ¥è¯†ç‚¹
    df_top5 = df_weak.head(5)

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown("### çŸ¥è¯†ç‚¹åˆ†å¸ƒ")

        # æ·»åŠ å›¾è¡¨åˆ‡æ¢é€‰é¡¹
        chart_type = st.radio("é€‰æ‹©å›¾è¡¨ç±»å‹", ["æŸ±çŠ¶å›¾", "é¥¼å›¾"], horizontal=True)

        if chart_type == "æŸ±çŠ¶å›¾":
            fig = px.bar(df_top5,
                         x='çŸ¥è¯†ç‚¹',
                         y='å‡ºç°æ¬¡æ•°',
                         color='å‡ºç°æ¬¡æ•°',
                         color_continuous_scale='Blues',
                         text='å‡ºç°æ¬¡æ•°')
            fig.update_layout(height=300)
        else:
            fig = px.pie(df_top5,
                         values='å‡ºç°æ¬¡æ•°',
                         names='çŸ¥è¯†ç‚¹',
                         hole=0.4,
                         color_discrete_sequence=px.colors.sequential.Blues_r)
            fig.update_traces(textposition='inside', textinfo='percent+label')

        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.markdown("### çŸ¥è¯†ç‚¹é›·è¾¾å›¾")

        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        categories = df_top5['çŸ¥è¯†ç‚¹'].tolist()
        values = df_top5['å‡ºç°æ¬¡æ•°'].tolist()

        # ç¡®ä¿é›·è¾¾å›¾é—­åˆ
        if len(categories) > 0:
            categories += categories[:1]
            values += values[:1]

        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            line=dict(color='royalblue'),
            name='å‡ºç°æ¬¡æ•°'
        ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, max(values) * 1.2 if len(values) > 0 else 1],
                    tickfont=dict(size=10)
                )
            ),
            height=350,
            margin=dict(l=50, r=50)
        )
        st.plotly_chart(fig, use_container_width=True)
        # æ–°å¢è¿”å›å€¼
        return {
            "top_weak_points": df_top5['çŸ¥è¯†ç‚¹'].tolist(),
            "weak_distribution": df_weak.set_index('çŸ¥è¯†ç‚¹')['å‡ºç°æ¬¡æ•°'].to_dict()
        }


# -----------------------------
# å­¦ä¹ è¡Œä¸ºåˆ†ææ¨¡å—
# -----------------------------
def analyze_learning_behavior(df):
    st.header("â° å­¦ä¹ è¡Œä¸ºåˆ†æ", divider="rainbow")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### å­¦ä¹ æ—¶æ®µåˆ†å¸ƒ")
        period_count = df['study_period'].value_counts()
        fig = px.pie(period_count,
                     values=period_count.values,
                     names=period_count.index,
                     hole=0.4,
                     color_discrete_sequence=px.colors.diverging.Portland)
        fig.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.markdown("### æé—®æ•°é‡åˆ†å¸ƒ")

        # ç”Ÿæˆåˆ†å¸ƒç›´æ–¹å›¾
        fig = px.histogram(df,
                           x='question_count',
                           nbins=8,
                           color_discrete_sequence=['#1f77b4'],
                           opacity=0.8)
        fig.update_layout(
            xaxis_title="æé—®æ•°é‡",
            yaxis_title="å­¦ç”Ÿäººæ•°",
            bargap=0.1,
            height=350
        )
        st.plotly_chart(fig, use_container_width=True)
        # æ–°å¢è¿”å›å€¼
        return {
            "period_dist": df['study_period'].value_counts(normalize=True).to_dict(),
            "question_stats": {
                "mean": df['question_count'].mean(),
                "std": df['question_count'].std()
            }
        }


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import plotly.express as px
import streamlit as st


def feature_engineering(df):
    """ç”Ÿæˆèšç±»åˆ†æç‰¹å¾ï¼Œå…³æ³¨æé—®æ¬¡æ•°å’Œè–„å¼±çŸ¥è¯†ç‚¹ï¼Œå¼±åŒ–å­¦ä¹ æ—¶æ®µ"""
    # ç‰¹å¾1ï¼šæé—®æ¬¡æ•°
    df['question_count'] = df['question_count']

    # ç‰¹å¾2ï¼šè–„å¼±çŸ¥è¯†ç‚¹æ•°é‡
    df['weak_count'] = df['weak_points'].apply(len)

    # ç‰¹å¾3ï¼šçŸ¥è¯†ç‚¹åµŒå…¥ï¼ˆä½¿ç”¨å…±ç°çŸ©é˜µå’ŒPCAé™ç»´ï¼‰
    all_points = [point for sublist in df['weak_points'] for point in sublist]
    unique_points = sorted(list(set(all_points)))
    co_matrix = pd.DataFrame(0, columns=unique_points, index=unique_points)

    # æ„å»ºçŸ¥è¯†ç‚¹å…±ç°çŸ©é˜µ
    for points in df['weak_points']:
        for i in range(len(points)):
            for j in range(i + 1, len(points)):
                p1, p2 = sorted([points[i], points[j]])
                co_matrix.loc[p1, p2] += 1
                co_matrix.loc[p2, p1] += 1

    # PCAé™ç»´åˆ°2ç»´
    pca = PCA(n_components=2)
    knowledge_embedding = pca.fit_transform(co_matrix)
    df['knowledge_emb1'] = df['weak_points'].apply(
        lambda x: np.mean([knowledge_embedding[unique_points.index(p)][0] for p in x], axis=0) if x else 0
    )
    df['knowledge_emb2'] = df['weak_points'].apply(
        lambda x: np.mean([knowledge_embedding[unique_points.index(p)][1] for p in x], axis=0) if x else 0
    )

    # åˆå¹¶ç‰¹å¾ï¼ˆä¸åŒ…å«å­¦ä¹ æ—¶æ®µï¼‰
    features = df[['question_count', 'weak_count', 'knowledge_emb1', 'knowledge_emb2']]
    return features


def analyze_student_clusters(df):
    st.header("ğŸ‘¥ å­¦ç”Ÿç¾¤ä½“åˆ†æ", divider="rainbow")

    # ç”Ÿæˆç‰¹å¾çŸ©é˜µ
    features = feature_engineering(df)

    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(features)

    # èšç±»å‚æ•°é€‰æ‹©
    col1, col2 = st.columns([1, 3])
    with col1:
        st.markdown("### èšç±»å‚æ•°è®¾ç½®")
        k = st.slider("é€‰æ‹©èšç±»æ•°é‡", 2, 5, 3)

        # æ‰§è¡ŒK-Meansèšç±»
        kmeans = KMeans(n_clusters=k, random_state=42)
        cluster_labels = kmeans.fit_predict(X_scaled)
        df['cluster'] = cluster_labels

        # è®¡ç®—è½®å»“ç³»æ•°
        silhouette_avg = silhouette_score(X_scaled, cluster_labels)
        st.metric("è½®å»“ç³»æ•°", f"{silhouette_avg:.2f}",
                  help="æ•°å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½ï¼Œå»ºè®®å€¼ï¼0.5")

    with col2:
        st.markdown("### èšç±»å¯è§†åŒ–ï¼ˆ3Dï¼‰")

        # ä½¿ç”¨PCAé™ç»´åˆ°3ç»´
        pca = PCA(n_components=3)
        reduced = pca.fit_transform(X_scaled)

        # å¯è§†åŒ–
        fig = px.scatter_3d(
            df,
            x=reduced[:, 0],
            y=reduced[:, 1],
            z=reduced[:, 2],
            color='cluster',
            hover_name='name',
            size='question_count',
            color_discrete_sequence=px.colors.qualitative.Vivid,
            opacity=0.8
        )
        fig.update_layout(
            scene=dict(
                xaxis_title='ä¸»æˆåˆ†1',
                yaxis_title='ä¸»æˆåˆ†2',
                zaxis_title='ä¸»æˆåˆ†3'
            ),
            height=600,
            margin=dict(l=0, r=0)
        )
        st.plotly_chart(fig, use_container_width=True)

        # æ˜¾ç¤ºåˆ†ç»„è¯¦æƒ…
        st.markdown("### åˆ†ç»„ç‰¹å¾åˆ†æ")
        clusters = sorted(df['cluster'].unique())
        cols = st.columns(len(clusters))

        for idx, col in enumerate(cols):
            with col:
                cluster_df = df[df['cluster'] == idx]
                color = px.colors.qualitative.Vivid[idx]

                # è®¡ç®—ç¾¤ç»„ç‰¹å¾
                top_weak = pd.Series(
                    [p for sublist in cluster_df['weak_points'] for p in sublist]
                ).value_counts().head(3).index.tolist()

                st.markdown(f"""
                    <div class="cluster-box" style="border-left:4px solid {color}">
                        <h4>ç¾¤ç»„ {idx} ({len(cluster_df)}äºº)</h4>
                        {', '.join(cluster_df['name'].tolist())}
                        <hr style="margin:8px 0">
                        <b>æ ¸å¿ƒç‰¹å¾ï¼š</b><br>
                        â€¢ å¹³å‡æé—®ï¼š{cluster_df['question_count'].mean():.1f}æ¬¡<br>
                        â€¢ å¹³å‡è–„å¼±ç‚¹ï¼š{cluster_df['weak_count'].mean():.1f}ä¸ª<br>
                        â€¢ é«˜é¢‘è–„å¼±ç‚¹ï¼š{', '.join(top_weak)}
                    </div>
                """, unsafe_allow_html=True)

        # -----------------------------
        # çŸ¥è¯†ç‚¹å…³è”åˆ†ææ¨¡å—
        # -----------------------------
        # æ–°å¢è¿”å›å€¼
                # æ–°å¢è¿”å›å€¼
        return {
                    "cluster_data": df[['name', 'cluster', 'question_count']],
                    "cluster_stats": {
                        "total_clusters": k,
                        "silhouette_score": round(silhouette_avg, 2),
                        "size_distribution": df['cluster'].value_counts().to_dict(),
                        "features": {
                            "question_mean": df.groupby('cluster')['question_count'].mean().to_dict(),
                            "weak_count_mean": df.groupby('cluster')['weak_count'].mean().to_dict()
                        }
                    }
                }


def analyze_knowledge_network(df):
    st.header("ğŸ”— çŸ¥è¯†ç‚¹å…³è”åˆ†æ", divider="rainbow")

    # æ„å»ºå…±ç°çŸ©é˜µ
    all_points = [point for sublist in df['weak_points'] for point in sublist]
    unique_points = sorted(list(set(all_points)))
    co_matrix = pd.DataFrame(0, columns=unique_points, index=unique_points)

    for points in df['weak_points']:
        for i in range(len(points)):
            for j in range(i + 1, len(points)):  # é¿å…é‡å¤è®¡ç®— A-B å’Œ B-A
                p1, p2 = sorted([points[i], points[j]])  # æ’åºï¼Œç¡®ä¿ A-B å’Œ B-A æ˜¯ç›¸åŒçš„ç»„åˆ
                co_matrix.loc[p1, p2] += 1
                co_matrix.loc[p2, p1] += 1  # å¯ä»¥å»æ‰ï¼Œä½¿ç”¨æ’åºä¿è¯ä¸€æ¬¡æ€§åŠ 1

    # --- è‡ªç„¶è¯­è¨€åˆ†ææŠ¥å‘Š ---
    st.markdown("### ğŸ“ å…³è”åˆ†æç»“è®º")


    # æœ€é«˜å…±ç°ç»„åˆæŸ¥è¯¢ä¼˜åŒ–
    max_co = co_matrix.stack().idxmax()
    max_value = co_matrix.loc[max_co[0], max_co[1]]

    # å­¤ç«‹çŸ¥è¯†ç‚¹æ£€æµ‹ä¼˜åŒ–
    isolated = [p for p in unique_points if (co_matrix[p].sum() + co_matrix.loc[p].sum()) == 0]

    # é«˜é¢‘çŸ¥è¯†ç‚¹ç»Ÿè®¡
    top_3 = Counter(all_points).most_common(3)
    top_3_str = ', '.join([f"{k} ({v}æ¬¡)" for k, v in top_3])

    analysis_report = f"""
       **å…³é”®å‘ç°ï¼š**
       1. æœ€å¼ºå…³è”ç»„åˆï¼š**{max_co[0]}** ä¸ **{max_co[1]}** å…±ç° {max_value} æ¬¡ï¼Œå»ºè®®ç»„åˆæ•™å­¦[4](@ref)
       2. é«˜é¢‘çŸ¥è¯†ç‚¹ TOP3ï¼š{top_3_str}
       3. å­¤ç«‹çŸ¥è¯†ç‚¹ï¼š{', '.join(isolated) if isolated else "æ— "}ï¼ˆéœ€å•ç‹¬å¼ºåŒ–ï¼‰
       4. ç½‘ç»œå¯†åº¦ï¼š{(co_matrix.sum().sum() / (len(unique_points) * (len(unique_points) - 1))) * 100:.1f}%
       5. å¹³å‡å…³è”å¼ºåº¦ï¼š{co_matrix.values.mean():.1f} ä¸ªç»„åˆ/çŸ¥è¯†ç‚¹
       """
    st.markdown(analysis_report)
    st.divider()

    # --- äº¤äº’å¼å¯è§†åŒ– ---
    col1, col2 = st.columns([1, 1])

    with col1:
        st.markdown("#### ğŸ“Š çŸ¥è¯†ç‚¹ç½‘ç»œ")
        # åˆ›å»º NetworkX å›¾
        # åˆ›å»º NetworkX å›¾
        G = nx.Graph()

        # æ·»åŠ èŠ‚ç‚¹
        for point in unique_points:
            G.add_node(point, size=all_points.count(point) * 10)

        # æ·»åŠ è¾¹ï¼ˆå…±ç°æ¬¡æ•°ä½œä¸ºæƒé‡ï¼‰
        for (i, j), weight in co_matrix.stack().items():
            if weight > 0:
                G.add_edge(i, j, weight=weight)

        # ç”ŸæˆèŠ‚ç‚¹çš„å¸ƒå±€
        pos = nx.spring_layout(G, seed=42)

        # æå–è¾¹çš„åæ ‡ä¿¡æ¯
        edge_x = []
        edge_y = []
        weights = []

        for edge in G.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            weights.append(edge[2]['weight'])

        # ä¿®æ­£ï¼šä½¿ç”¨å•ä¸ªæ•°å€¼ä½œä¸º widthï¼ˆå¹³å‡å®½åº¦ï¼‰
        edge_width = max(1.0, np.mean(weights) * 0.5)  # ç¡®ä¿ width æ˜¯ float
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=edge_width, color='#888'),  # è¿™é‡Œæ˜¯ä¸€ä¸ª floatï¼Œè€Œä¸æ˜¯ list
            hoverinfo='none',
            mode='lines')

        node_sizes = [G.nodes[n]['size'] for n in G.nodes]
        node_x = [pos[n][0] for n in G.nodes]
        node_y = [pos[n][1] for n in G.nodes]
        node_text = [f"{n}<br>å‡ºç°æ¬¡æ•°: {G.nodes[n]['size'] // 10}" for n in G.nodes]

        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="top center",
            marker=dict(
                size=np.sqrt(node_sizes),
                color=node_sizes,
                colorscale='Blues',
                line_width=2))

        fig = go.Figure(data=[edge_trace, node_trace],
                        layout=go.Layout(
                            showlegend=False,
                            hovermode='closest',
                            margin=dict(b=20, l=5, r=5, t=40),
                            height=600))
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.markdown("#### ğŸ“ˆ å…³è”çƒ­åŠ›å›¾")

        # ä½¿ç”¨Plotlyäº¤äº’å¼çƒ­åŠ›å›¾
        fig = px.imshow(
            co_matrix,
            labels=dict(x="çŸ¥è¯†ç‚¹", y="çŸ¥è¯†ç‚¹", color="å…±ç°æ¬¡æ•°"),
            color_continuous_scale='YlGnBu',
            aspect="auto"
        )
        fig.update_xaxes(tickangle=45)
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)

        # --- ç»„åˆæ¨èç³»ç»Ÿä¼˜åŒ– ---
    st.markdown("### ğŸ¯ æ¨èæ•™å­¦ç»„åˆ")
    co_occurrences = co_matrix.stack().reset_index()
    co_occurrences.columns = ['çŸ¥è¯†ç‚¹A', 'çŸ¥è¯†ç‚¹B', 'æ¬¡æ•°']
    co_occurrences = co_occurrences[
        (co_occurrences['æ¬¡æ•°'] > 0) &
        (co_occurrences['çŸ¥è¯†ç‚¹A'] < co_occurrences['çŸ¥è¯†ç‚¹B'])  # æ’é™¤é‡å¤æ’åˆ—
        ].sort_values('æ¬¡æ•°', ascending=False).reset_index(drop=True)

    top_combos = co_occurrences.head(10)

    for idx, row in top_combos.iterrows():
        st.markdown(f"""
           - **{row['çŸ¥è¯†ç‚¹A']} + {row['çŸ¥è¯†ç‚¹B']}**  
             å…±ç°å¼ºåº¦ï¼š{row['æ¬¡æ•°']} æ¬¡ï¼ˆTOP{idx + 1}ï¼‰  
           """)
# --- å…±ç°ç»„åˆå…¨é‡æ¸…å• ---
    st.markdown("### ğŸ“‹ å…±ç°ç»„åˆå…¨é‡æ¸…å•")
    st.dataframe(
            co_occurrences,
            column_config={
                "çŸ¥è¯†ç‚¹A": st.column_config.TextColumn("èµ·ç‚¹çŸ¥è¯†ç‚¹"),
                "çŸ¥è¯†ç‚¹B": st.column_config.TextColumn("å…³è”çŸ¥è¯†ç‚¹"),
                "æ¬¡æ•°": st.column_config.NumberColumn("å…±ç°å¼ºåº¦", format="%dæ¬¡")
            },
            height=400,
            use_container_width=True,
            hide_index=True
        )
    # æ–°å¢è¿”å›å€¼
    return {
        "co_occurrence_matrix": co_matrix,
        "top_combinations": top_combos[['çŸ¥è¯†ç‚¹A', 'çŸ¥è¯†ç‚¹B', 'æ¬¡æ•°']].values.tolist(),
        "network_density": co_matrix.sum().sum() / (len(unique_points) * (len(unique_points) - 1))
    }

def format_student_data(df):
    """å°†å­¦ç”Ÿæ•°æ®æ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
    examples = []
    for _, row in df.sample(min(3, len(df))).iterrows():
        desc = (
            f"å­¦ç”Ÿ{row['name']}ï¼š"
            f"è–„å¼±çŸ¥è¯†ç‚¹åŒ…æ‹¬{'ã€'.join(row['weak_points'])}, "
            f"ä¸»è¦åœ¨{row['study_period']}å­¦ä¹ ï¼Œ"
            f"å¹³å‡æ¯å‘¨æé—®{row['question_count']}æ¬¡"
        )
        examples.append(desc)
    return "\n".join(examples)
def generate_analysis_report(df, analysis_results):
    """ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"""
    st.header("ğŸ§  AI æ·±åº¦åˆ†ææŠ¥å‘Š", divider="rainbow")

    # æ•°æ®æ ¡éªŒ
    try:
        validate_analysis_data(analysis_results)
    except Exception as e:
        st.error(f"æ•°æ®æ ¡éªŒå¤±è´¥: {str(e)}")
        return
    # æ•°æ®é¢„å¤„ç†
    raw_data_sample = format_student_data(df)
    # æ„å»ºæç¤ºæ¨¡æ¿
    template = """ä½œä¸ºæ•™è‚²åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ•°æ®ç”Ÿæˆæ•™å­¦åˆ†ææŠ¥å‘Šï¼š

## æ•°æ®æ¦‚è§ˆ
- å­¦ç”Ÿæ€»æ•°ï¼š{student_count}äºº
- å¹³å‡æé—®ï¼š{question_mean}æ¬¡/äººï¼ˆæ ‡å‡†å·®ï¼š{question_std}ï¼‰
- ä¸»è¦è–„å¼±ç‚¹ï¼š{top_weak}
- æé—®æ—¶æ®µåˆ†å¸ƒï¼š{period_dist}
- ç­çº§å­¦ä¹ æƒ…å†µåˆ†å±‚ï¼š{cluster_info}

## åˆ†æè¦æ±‚
1. ç°çŠ¶åˆ†æï¼šåŸºäºä¸Šé¢çš„æ•°æ®åˆ†æç­çº§ç°çŠ¶ï¼Œè¯´æ˜ç­çº§ç‰¹å¾ã€‚
2. é—®é¢˜è¯Šæ–­ï¼šåŸºäºå·²æœ‰ä¿¡æ¯ï¼š
   -æŒ‡å‡ºè¯¥ç­çº§çš„**2-3ä¸ªæ•™å­¦ç—›ç‚¹**ï¼Œ 
   -åŸºäºçŸ¥è¯†ç‚¹å…³è”çš„3ä¸ªæ•™å­¦ä¼˜åŒ–ç‚¹
   - é«˜é¢‘è–„å¼±ç‚¹çš„çªç ´ç­–ç•¥
   - å­¤ç«‹çŸ¥è¯†ç‚¹çš„æ•™å­¦èå…¥æ–¹æ¡ˆ
3.ç¾¤ä½“ç‰¹å¾è§£æï¼š
   - ä¸åŒå­¦ä¹ æ—¶æ®µå­¦ç”Ÿçš„æé—®æ¨¡å¼å·®å¼‚
   - å„ç¾¤ç»„ç‰¹å¾å¯¹æ•™å­¦ç­–ç•¥çš„å¯ç¤º
   - ç¾¤ç»„é—´çŸ¥è¯†ç‚¹æŒæ¡å·®å¼‚åˆ†æ
4. æ•™å­¦å»ºè®®ï¼šæä¾›**å…·ä½“æ–¹æ¡ˆ**ï¼ŒåŒ…å«ï¼š
   - é’ˆå¯¹ä¸åŒåˆ†ç»„ï¼Œç»™å‡ºé€‚åˆä»–ä»¬çš„æ•™å­¦ç­–ç•¥ï¼Œè¦æ±‚è¯¦ç»†å¯å®ç°å…·ä½“ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„æ•™å­¦æªæ–½å’Œå»ºè®®ã€‚
   - æ ¹æ®å­¦ä¹ æ—¶æ®µçš„èµ„æºåˆ†é…å»ºè®®ã€‚
   - åŸºäºçŸ¥è¯†å…³è”æ€§ï¼Œå¸®åŠ©æ•™å¸ˆä¼˜åŒ–æ•™å­¦å®‰æ’ã€‚
   -åˆ†æç›®å‰ç­çº§çš„å­˜åœ¨çš„æ•™å­¦ä¸Šçš„é—®é¢˜å’Œç¼ºé™·
   -åŸºäºç¾¤ä½“ç‰¹å¾ï¼Œè¯´æ˜å„åˆ†ç»„çš„å·®å¼‚åŒ–æ•™å­¦éœ€æ±‚
   -æŒ‡å¯¼è€å¸ˆå¦‚ä½•åˆ©ç”¨é«˜é¢‘ç»„åˆè®¾è®¡ç»¼åˆç»ƒä¹ é¢˜ä»¥åŠåŠ å¼ºå¯¹å­¤ç«‹çŸ¥è¯†ç‚¹çš„å¼ºåŒ–æ–¹æ¡ˆ
4 åŸºäºè¢æœ¯å­¦ç”Ÿæ•°æ®{raw_data_sample}æ›´æ·±å±‚æ¬¡æŒ–æ˜æœ‰ä»·å€¼çš„æ•™å­¦ä¿¡æ¯ï¼Œåé¦ˆç»™è€å¸ˆï¼Œæ³¨æ„ä¸è¦ä¸å‰é¢çš„ä¿¡æ¯é‡å¤ã€‚
5. ç‰¹æ®Šå…³æ³¨ï¼šåˆ—å‡ºéœ€è¦é‡ç‚¹å…³æ³¨çš„å­¦å‘˜åŠåŸå› 

## é™„åŠ æ•°æ®
- çŸ¥è¯†ç‚¹ç½‘ç»œå¯†åº¦ï¼š{density}ï¼ˆè¶Šé«˜è¡¨ç¤ºå…³è”è¶Šç´§å¯†ï¼‰
- å‰3çŸ¥è¯†ç‚¹ç»„åˆï¼š{top_combinations}
- å­¤ç«‹çŸ¥è¯†ç‚¹ï¼š{isolated}

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„ä¸­æ–‡æ’°å†™,ä¸”å†…å®¹å°½å¯èƒ½è¯¦å°½ï¼Œä¸°å¯Œï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«åˆ†çº§æ ‡é¢˜å’Œé‡ç‚¹æ ‡è®°ã€‚"""

    # å‡†å¤‡æ¨¡æ¿å‚æ•°
    params = {
        "student_count": len(df),
        "question_mean": analysis_results['question_stats']['mean'],
        "question_std": analysis_results['question_stats']['std'],
        "top_weak": ", ".join(analysis_results['top_weak_points'][:3]),
        "period_dist": "\n".join([f"- {k}: {v * 100:.1f}%" for k, v in analysis_results['study_period_dist'].items()]),
        "cluster_info": f"{analysis_results['cluster_summary']['total_clusters']}ä¸ªåˆ†ç»„ï¼Œæœ€å¤§ç»„{max(analysis_results['cluster_summary']['cluster_dist'].values())}äºº",
        "density": analysis_results['knowledge_network']['network_density'],
        "top_combinations": "\n".join(
            [f"{a}+{b}ï¼ˆ{c}æ¬¡ï¼‰" for a, b, c in analysis_results['knowledge_network']['top_combinations'][:3]]),
        "isolated": "æ— " if not analysis_results['knowledge_network'].get('isolated_points') else ", ".join(
            analysis_results['knowledge_network']['isolated_points']),
        "raw_data_sample":raw_data_sample
    }

    # åˆå§‹åŒ–å¤§æ¨¡å‹
    llm = Tongyi(temperature=0.7,top_p=0.9)
    prompt = PromptTemplate(template=template, input_variables=list(params.keys()))
    chain = LLMChain(llm=llm, prompt=prompt)

    # ç”ŸæˆæŠ¥å‘Š
    with st.spinner('AIæ­£åœ¨ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š...'):
        try:
            report = chain.run(**params)
            st.markdown(report, unsafe_allow_html=True)

            # æ·»åŠ é‡æ–°ç”ŸæˆæŒ‰é’®
            if st.button("ğŸ”„ é‡æ–°ç”ŸæˆæŠ¥å‘Š"):
                report = chain.run(**params)
                st.markdown(report, unsafe_allow_html=True)

        except RuntimeError as e:
            st.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        except Exception as e:
            st.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}")